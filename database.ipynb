{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install psycopg2\n",
    "%pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateDatabase",
     "evalue": "database \"angeliteforecast\" already exists\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDuplicateDatabase\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Create the database\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mcreate_database\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mangeliteforecast\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpostgres\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthesis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m, in \u001b[0;36mcreate_database\u001b[1;34m(db_name, username, password, host, port)\u001b[0m\n\u001b[0;32m      7\u001b[0m conn\u001b[38;5;241m.\u001b[39mset_isolation_level(psycopg2\u001b[38;5;241m.\u001b[39mextensions\u001b[38;5;241m.\u001b[39mISOLATION_LEVEL_AUTOCOMMIT)\n\u001b[0;32m      8\u001b[0m cur \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[1;32m----> 9\u001b[0m \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCREATE DATABASE \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdb_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m cur\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     11\u001b[0m conn\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mDuplicateDatabase\u001b[0m: database \"angeliteforecast\" already exists\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Function to create a new database\n",
    "def create_database(db_name, username, password, host='localhost', port=5432):\n",
    "    conn = psycopg2.connect(dbname=\"postgres\", user=username, password=password, host=host, port=port)\n",
    "    conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"CREATE DATABASE {db_name};\")\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "# Create the database\n",
    "create_database('angeliteforecast', 'postgres', 'thesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "def connect_to_database(username, password, host='34.126.160.94', port=5432, db_name='angeliteforecast'):\n",
    "    \"\"\"Create a database connection.\"\"\"\n",
    "    connection_string = f'postgresql://{username}:{password}@{host}:{port}/{db_name}'\n",
    "    engine = create_engine(connection_string)\n",
    "    return engine\n",
    "engine = connect_to_database(\"postgres\", \"thesis\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "similar_majors_dict = {\n",
    "    # CCJEF\n",
    "    \"Criminology\": [\"BS Criminology\"], # Combine Criminology and BS Criminology\n",
    "\n",
    "    \n",
    "    # SAS\n",
    "    \"ABComm\": [\"BA Comm\"],\n",
    "    \"CommArts\": [\"BA Comm\"],\n",
    "    \"AdvPublicRel\": [\"BA Comm\"], # Area of Specialization\n",
    "    # \"CommArts\": \"BA Comm\",     # Lack of Data | Drop\n",
    "\n",
    "\n",
    "    # SBA\n",
    "    \"Accounting\": [\"Accountancy\"],\n",
    "    \"Accounting Tech\": [\"BSAcctgInfoSys\"],\n",
    "    \"BSBA-BM-Marketi\": [\"BusMgt\", \"BSBA-MktgMgmt\"],\n",
    "    \"BussMgmt-HRM\": [\"BusMgt\", \"BSBA-HRMgmt\"],\n",
    "\n",
    "\n",
    "    # SEA\n",
    "    \"ECE\": [\"ELECENG\"],\n",
    "    # \"BSCE-CEM\": [\"CE\", \"BSCE-CEM\"],\n",
    "    # \"BSCE-SE\": [\"CE\", \"BSCE-SE\"],\n",
    "    # \"BSCE-TE\": [\"CE\", \"BSCE-TE\"],\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # SED\n",
    "    \"BEED-MajSPED\": [\"BSNEd\"],           # Name change?\n",
    "    \"BEED-SpecialEdu\": [\"BSNEd\"],        # Name change?\n",
    "    \"BPE-SPE\": [\"BPEd\"],                   # Name change?\n",
    "    \"BSED-ValEd\": [\"BSED-RelValEd\"],\n",
    "    \"BSMath\": [\"Math\"],\n",
    "    \n",
    "\n",
    "    # SHTM\n",
    "    \"BS EventMgmt\": [\"BSTM-Events\"],\n",
    "\n",
    "\n",
    "    # SOC\n",
    "    \"BSCSSysDev\": [\"BSCompsci\"],              # New Curriculum\n",
    "    \"BSITAnimation\": [\"BSEMC-DA\"],            # New Curriculum\n",
    "    \"BSITAreaAnimati\": [\"BSEMC-DA\"],          # New Curriculum\n",
    "    \"BSCyberplusPSM\": [\"BSCybersecurity\"],    # Same Major but with Professional Science Master's Degree\n",
    "    # \"BSInfoTech\": \"BSIT\",                 # General IT Major | Lack of Data | Drop\n",
    "    # \"BSITMultiTec\": \"BSIT\",               # Discontinued | Lack of Data | Drop\n",
    "    \"BSITAreaNetAdmi\": [\"BSITNetAdmin\"], \n",
    "    \"BSITAreaWebDev\": [\"BSITWebdev\"],\n",
    "\n",
    "    }\n",
    "\n",
    "# Dictionary for major with incorrect department\n",
    "incorrect_department_dict = {\n",
    "    \"BSBA-HRM\": \"SBA\",\n",
    "    # \"MAPEH-BSED\": \"SED\",\n",
    "}\n",
    "\n",
    "department_dict = {\n",
    "    \"CHTM\" : \"SHTM\",\n",
    "    \"CICT\" : \"SOC\",\n",
    "}\n",
    "\n",
    "# List of majors to drop\n",
    "drop_majors = [\n",
    "    # MA\n",
    "    # \"MAPEH-BSED\",     # Lack of data | Discontinued | SED Department?\n",
    "    \"MBM\",              # Lack of data | only 1 student\n",
    "    \"MSEngMgmt\",        # Lack of data | only 1 student\n",
    "    \n",
    "\n",
    "    # SAS\n",
    "    # \"CommArts\",         # Lack of data | only 1 student\n",
    "    \"LanguageLit\",      # Lack of data | only 1 student\n",
    "\n",
    "    # SED\n",
    "    \n",
    "    # SHTM\n",
    "    \"BSBATourism\",\n",
    "    \"BSTM-Tourism\",     # Lack of data | BSTourism? | only 2 students\n",
    "\n",
    "\n",
    "    # SOC\n",
    "    \"AssCompSci\",       # Lack of data | only 1 student\n",
    "    \"BSITMultiTec\",     # Lack of data | Discontinued\n",
    "    \"BSInfoTech\",       # Lack of data | only 1 student\n",
    "\n",
    "    # SEA\n",
    "    \"ECETech.\",        # Lack of data | Discontinued\n",
    "\n",
    "\n",
    "    # Extras | Aggregate Counts\n",
    "    \"TOTAL\",\n",
    "    \"GRAND TOTAL\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \n",
    "\n",
    "    # Step 2: Correct departments\n",
    "\n",
    "    for major, department in incorrect_department_dict.items():\n",
    "        df.loc[df[\"Major\"]== major, 'Department'] = department\n",
    "        \n",
    "    # Step 3: Rename departments\n",
    "    df['Department'] = df['Department'].replace(department_dict)\n",
    "    # Step 4: If duplicate, get sum\n",
    "    numerical_features = df.loc[:, \"1st_Year\":].columns\n",
    "    df = df.groupby(['Start_Year', 'Semester', 'Department', 'Major'], as_index=False).agg({feature: 'sum' for feature in numerical_features})\n",
    "    # Step 4: Drop unwanted majors\n",
    "    # df = df[~df['Major'].isin(drop_majors)]\n",
    "\n",
    "    # df = df[~df['Department'].isin(['GS', 'JHS', 'HAUSPELL', 'HAU', 'MA'])]\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.sort_values(by=[\"Start_Year\", \"Semester\", \"Department\", \"Major\"])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Major Department  Semester  Start_Year  End_Year  1st_Year  2nd_Year  \\\n",
      "67    TOTAL        SOC         1        2016      2017      76.0     456.0   \n",
      "134   TOTAL        SOC         2        2016      2017      25.0     431.0   \n",
      "207   TOTAL        SOC         1        2017      2018       7.0      89.0   \n",
      "275   TOTAL        SOC         2        2017      2018       3.0      49.0   \n",
      "352   TOTAL        SOC         1        2018      2019     244.0      26.0   \n",
      "421   TOTAL        SOC         2        2018      2019     238.0      11.0   \n",
      "494   TOTAL        SOC         1        2019      2020     326.0     220.0   \n",
      "561   TOTAL        SOC         2        2019      2020     298.0     216.0   \n",
      "632   TOTAL        SOC         1        2020      2021     216.0     256.0   \n",
      "697   TOTAL        SOC         2        2020      2021     196.0     222.0   \n",
      "773   TOTAL        SOC         1        2021      2022     324.0     219.0   \n",
      "842   TOTAL        SOC         2        2021      2022     299.0     197.0   \n",
      "916   TOTAL        SOC         1        2022      2023     359.0     282.0   \n",
      "988   TOTAL        SOC         2        2022      2023     324.0     265.0   \n",
      "1060  TOTAL        SOC         1        2023      2024     472.0     323.0   \n",
      "1129  TOTAL        SOC         2        2023      2024     458.0     318.0   \n",
      "1204  TOTAL        SOC         1        2024      2025     462.0     443.0   \n",
      "\n",
      "      3rd_Year  4th_Year  5th_Year  ...  Grade_4  Grade_5  Grade_6  Grade_7  \\\n",
      "67       326.0     281.0       NaN  ...      NaN      NaN      NaN      NaN   \n",
      "134      342.0     264.0       NaN  ...      NaN      NaN      NaN      NaN   \n",
      "207      421.0     272.0       NaN  ...      NaN      NaN      NaN      NaN   \n",
      "275      398.0     286.0       NaN  ...      NaN      NaN      NaN      NaN   \n",
      "352       94.0     381.0       NaN  ...      NaN      NaN      NaN      NaN   \n",
      "421       24.0     413.0       NaN  ...      NaN      NaN      NaN      NaN   \n",
      "494        8.0     120.0       NaN  ...      NaN      NaN      NaN      NaN   \n",
      "561        1.0      55.0       NaN  ...      NaN      NaN      NaN      NaN   \n",
      "632      172.0       7.0       NaN  ...      NaN      NaN      NaN      NaN   \n",
      "697      172.0       7.0       NaN  ...      NaN      NaN      NaN      NaN   \n",
      "773      215.0     160.0       NaN  ...      NaN      NaN      NaN      NaN   \n",
      "842      221.0     169.0       NaN  ...      NaN      NaN      NaN      NaN   \n",
      "916      200.0     216.0       NaN  ...      NaN      NaN      NaN      NaN   \n",
      "988      192.0     217.0       NaN  ...      NaN      NaN      NaN      NaN   \n",
      "1060     268.0     200.0       NaN  ...      NaN      NaN      NaN      NaN   \n",
      "1129     253.0     196.0       NaN  ...      NaN      NaN      NaN      NaN   \n",
      "1204     294.0     262.0       NaN  ...      NaN      NaN      NaN      NaN   \n",
      "\n",
      "      Grade_8  Grade_9  Grade_10  Grade_11  Grade_12  TOTAL  \n",
      "67        NaN      NaN       NaN       NaN       NaN   1139  \n",
      "134       NaN      NaN       NaN       NaN       NaN   1062  \n",
      "207       NaN      NaN       NaN       NaN       NaN    789  \n",
      "275       NaN      NaN       NaN       NaN       NaN    736  \n",
      "352       NaN      NaN       NaN       NaN       NaN    745  \n",
      "421       NaN      NaN       NaN       NaN       NaN    686  \n",
      "494       NaN      NaN       NaN       NaN       NaN    674  \n",
      "561       NaN      NaN       NaN       NaN       NaN    570  \n",
      "632       NaN      NaN       NaN       NaN       NaN    651  \n",
      "697       NaN      NaN       NaN       NaN       NaN    597  \n",
      "773       NaN      NaN       NaN       NaN       NaN    918  \n",
      "842       NaN      NaN       NaN       NaN       NaN    886  \n",
      "916       NaN      NaN       NaN       NaN       NaN   1057  \n",
      "988       NaN      NaN       NaN       NaN       NaN    998  \n",
      "1060      NaN      NaN       NaN       NaN       NaN   1263  \n",
      "1129      NaN      NaN       NaN       NaN       NaN   1225  \n",
      "1204      NaN      NaN       NaN       NaN       NaN   1461  \n",
      "\n",
      "[17 rows x 23 columns]\n",
      "      Start_Year  Semester Department  Major  1st_Year  2nd_Year  3rd_Year  \\\n",
      "66          2016         1        SOC  TOTAL      77.0     456.0     326.0   \n",
      "131         2016         2        SOC  TOTAL      26.0     432.0     342.0   \n",
      "203         2017         1        SOC  TOTAL       8.0      90.0     422.0   \n",
      "270         2017         2        SOC  TOTAL       3.0      51.0     399.0   \n",
      "347         2018         1        SOC  TOTAL     244.0      26.0      94.0   \n",
      "416         2018         2        SOC  TOTAL     238.0      11.0      24.0   \n",
      "489         2019         1        SOC  TOTAL     326.0     220.0       8.0   \n",
      "555         2019         2        SOC  TOTAL     299.0     216.0       1.0   \n",
      "626         2020         1        SOC  TOTAL     216.0     256.0     172.0   \n",
      "691         2020         2        SOC  TOTAL     196.0     222.0     172.0   \n",
      "767         2021         1        SOC  TOTAL     324.0     219.0     215.0   \n",
      "836         2021         2        SOC  TOTAL     299.0     197.0     221.0   \n",
      "910         2022         1        SOC  TOTAL     359.0     282.0     200.0   \n",
      "982         2022         2        SOC  TOTAL     324.0     265.0     192.0   \n",
      "1054        2023         1        SOC  TOTAL     472.0     323.0     268.0   \n",
      "1123        2023         2        SOC  TOTAL     458.0     318.0     253.0   \n",
      "1198        2024         1        SOC  TOTAL     462.0     443.0     294.0   \n",
      "\n",
      "      4th_Year  5th_Year  Grade_1  ...  Grade_4  Grade_5  Grade_6  Grade_7  \\\n",
      "66       282.0       0.0      0.0  ...      0.0      0.0      0.0      0.0   \n",
      "131      264.0       0.0      0.0  ...      0.0      0.0      0.0      0.0   \n",
      "203      273.0       0.0      0.0  ...      0.0      0.0      0.0      0.0   \n",
      "270      286.0       0.0      0.0  ...      0.0      0.0      0.0      0.0   \n",
      "347      381.0       0.0      0.0  ...      0.0      0.0      0.0      0.0   \n",
      "416      413.0       0.0      0.0  ...      0.0      0.0      0.0      0.0   \n",
      "489      120.0       0.0      0.0  ...      0.0      0.0      0.0      0.0   \n",
      "555       55.0       0.0      0.0  ...      0.0      0.0      0.0      0.0   \n",
      "626        7.0       0.0      0.0  ...      0.0      0.0      0.0      0.0   \n",
      "691        7.0       0.0      0.0  ...      0.0      0.0      0.0      0.0   \n",
      "767      160.0       0.0      0.0  ...      0.0      0.0      0.0      0.0   \n",
      "836      169.0       0.0      0.0  ...      0.0      0.0      0.0      0.0   \n",
      "910      216.0       0.0      0.0  ...      0.0      0.0      0.0      0.0   \n",
      "982      217.0       0.0      0.0  ...      0.0      0.0      0.0      0.0   \n",
      "1054     200.0       0.0      0.0  ...      0.0      0.0      0.0      0.0   \n",
      "1123     196.0       0.0      0.0  ...      0.0      0.0      0.0      0.0   \n",
      "1198     262.0       0.0      0.0  ...      0.0      0.0      0.0      0.0   \n",
      "\n",
      "      Grade_8  Grade_9  Grade_10  Grade_11  Grade_12  TOTAL  \n",
      "66        0.0      0.0       0.0       0.0       0.0   1141  \n",
      "131       0.0      0.0       0.0       0.0       0.0   1064  \n",
      "203       0.0      0.0       0.0       0.0       0.0    793  \n",
      "270       0.0      0.0       0.0       0.0       0.0    739  \n",
      "347       0.0      0.0       0.0       0.0       0.0    745  \n",
      "416       0.0      0.0       0.0       0.0       0.0    686  \n",
      "489       0.0      0.0       0.0       0.0       0.0    674  \n",
      "555       0.0      0.0       0.0       0.0       0.0    571  \n",
      "626       0.0      0.0       0.0       0.0       0.0    651  \n",
      "691       0.0      0.0       0.0       0.0       0.0    597  \n",
      "767       0.0      0.0       0.0       0.0       0.0    918  \n",
      "836       0.0      0.0       0.0       0.0       0.0    886  \n",
      "910       0.0      0.0       0.0       0.0       0.0   1057  \n",
      "982       0.0      0.0       0.0       0.0       0.0    998  \n",
      "1054      0.0      0.0       0.0       0.0       0.0   1263  \n",
      "1123      0.0      0.0       0.0       0.0       0.0   1225  \n",
      "1198      0.0      0.0       0.0       0.0       0.0   1461  \n",
      "\n",
      "[17 rows x 22 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/Enrollment_Data.csv\").sort_values(by=[\"Start_Year\", \"Semester\", \"Department\", \"Major\"])\n",
    "cpi_df = pd.read_csv(\"data/CPI_Education.csv\").sort_values(by=[\"Year\", \"Month\"])\n",
    "inflation_df = pd.read_csv(\"data/Inflation_Rate.csv\").sort_values(by=\"Start_Year\")\n",
    "admission_df = pd.read_csv(\"data/Admission_Data.csv\").sort_values(by=[\"Start_Year\", \"Department\"])\n",
    "admission_df[\"Department\"] = admission_df[\"Department\"].replace(\"CICT\", \"SOC\")\n",
    "\n",
    "hfce_df = pd.read_csv(\"data/HFCE.csv\").sort_values(by=[\"Start_Year\", \"Quarter\"])\n",
    "print(df[(df[\"Department\"] == \"SOC\") & (df[\"Major\"] == \"TOTAL\")])\n",
    "\n",
    "df = clean_data(df)\n",
    "print(df[(df[\"Department\"] == \"SOC\") & (df[\"Major\"] == \"TOTAL\")])\n",
    "\n",
    "# Upload data to database\n",
    "df.to_sql('enrollment', engine, if_exists='replace', index=False)\n",
    "cpi_df.to_sql('cpi_education', engine, if_exists='replace', index=False)\n",
    "inflation_df.to_sql('inflation_rate', engine, if_exists='replace', index=False)\n",
    "admission_df.to_sql('admission', engine, if_exists='replace', index=False)\n",
    "hfce_df.to_sql('hfce', engine, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "def create_lag_features(data, lag_steps=1, group=[\"Major\"], target=\"1st_Year\"):\n",
    "    if group:\n",
    "        for i in range(1, lag_steps + 1):\n",
    "            data[f'{target}_lag_{i}'] = data.groupby(group)[target].shift(i)\n",
    "    else:\n",
    "        for i in range(1, lag_steps + 1):\n",
    "            data[f'{target}_lag_{i}'] = data[target].shift(i)\n",
    "    return data\n",
    "\n",
    "def create_rolling_mean(data, group=[\"Major\"], window_size=3, target=\"1st_Year\", min_periods=1, lag_steps=0):\n",
    "    lag_string = f'_lag_{lag_steps}' if lag_steps else \"\"\n",
    "    if group:\n",
    "        data[f'{target}_rolling_mean{lag_string}'] = data.groupby(group)[target].shift(lag_steps).rolling(window=window_size, min_periods=min_periods).mean().values\n",
    "    else:\n",
    "        data[f'{target}_rolling_mean{lag_string}'] = data[target].shift(lag_steps).rolling(window=window_size, min_periods=min_periods).mean()\n",
    "    return data\n",
    "\n",
    "def create_rolling_std(data, group=[\"Major\"], window_size=3, target=\"1st_Year\", min_periods=1, lag_steps=0):\n",
    "    lag_string = f'_lag_{lag_steps}' if lag_steps else \"\"\n",
    "    if group:\n",
    "        data[f'{target}_rolling_std{lag_string}'] = data.groupby(group)[target].shift(lag_steps).rolling(window=window_size, min_periods=min_periods).std().values\n",
    "    else:\n",
    "        data[f'{target}_rolling_std{lag_string}'] = data[target].shift(lag_steps).rolling(window=window_size, min_periods=min_periods).std()\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return np.mean(2 * np.abs((y_true - y_pred) /  (np.abs(y_true) + np.abs(y_pred)))) * 100\n",
    "\n",
    "# MAPE\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) /  y_true)) * 100\n",
    "\n",
    "\n",
    "def modelresults(y_true, predictions):\n",
    "    mae = mean_absolute_error(y_true, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, predictions))\n",
    "    r2 = r2_score(y_true, predictions)\n",
    "    smape_score = smape(y_true, predictions)\n",
    "    mape_score = mape(y_true, predictions)\n",
    "\n",
    "    print(f\"Symmetric Mean Absolute Percentage Error: {smape_score:.2f}%\")\n",
    "    print(f'Mean Absolute Percentage Error: {mape_score:.2f}%')\n",
    "    print(f'Mean Absolute Error: {mae:.4f}')\n",
    "    print(f'Root Mean Squared Error: {rmse:.4f}')\n",
    "    print(f'R2 Score: {r2*100:.4f}')\n",
    "\n",
    "    return {\n",
    "        \"smape\": smape_score,\n",
    "        \"mape\": mape_score,\n",
    "        \"mae\": mae,\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\": r2\n",
    "    }\n",
    "\n",
    "\n",
    "# Determine the start of enrollment period\n",
    "def determine_start_month(semester):\n",
    "    if semester == 1:\n",
    "       return 5  # June\n",
    "    else:\n",
    "        \n",
    "        return 10  # November"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Local\\temp\\ipykernel_8108\\2511560472.py:6: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables for users, roles, and user_roles have been created.\n",
      "Admin role added.\n",
      "User role added.\n",
      "Admin and User roles have been added to the database.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, ForeignKey, Table\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship, sessionmaker\n",
    "from sqlalchemy.sql import func\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "# Define the association table\n",
    "user_roles = Table('user_roles', Base.metadata,\n",
    "    Column('user_id', Integer, ForeignKey('users.id')),\n",
    "    Column('role_id', Integer, ForeignKey('roles.id'))\n",
    ")\n",
    "\n",
    "class User(Base):\n",
    "    __tablename__ = 'users'\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    username = Column(String(50), unique=True, nullable=False)\n",
    "    email = Column(String(100), unique=True, nullable=False)\n",
    "    password_hash = Column(String(255), nullable=False)\n",
    "    created_at = Column(DateTime(timezone=True), server_default=func.now())\n",
    "    updated_at = Column(DateTime(timezone=True), onupdate=func.now())\n",
    "    login_counter = Column(Integer, default=0)\n",
    "\n",
    "\n",
    "    roles = relationship('Role', secondary=user_roles, back_populates='users')\n",
    "\n",
    "class Role(Base):\n",
    "    __tablename__ = 'roles'\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String(50), unique=True, nullable=False)\n",
    "    description = Column(Text)\n",
    "\n",
    "    users = relationship('User', secondary=user_roles, back_populates='roles')\n",
    "\n",
    "# Create the tables\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "print(\"Tables for users, roles, and user_roles have been created.\")\n",
    "\n",
    "# Create a session\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Add Admin and User roles\n",
    "admin_role = Role(name='Admin', description='Administrator role with full access')\n",
    "user_role = Role(name='User', description='Standard user role with limited access')\n",
    "\n",
    "# Check if roles already exist\n",
    "existing_admin = session.query(Role).filter_by(name='Admin').first()\n",
    "existing_user = session.query(Role).filter_by(name='User').first()\n",
    "\n",
    "if not existing_admin:\n",
    "    session.add(admin_role)\n",
    "    print(\"Admin role added.\")\n",
    "else:\n",
    "    print(\"Admin role already exists.\")\n",
    "\n",
    "if not existing_user:\n",
    "    session.add(user_role)\n",
    "    print(\"User role added.\")\n",
    "else:\n",
    "    print(\"User role already exists.\")\n",
    "\n",
    "# Commit the changes\n",
    "session.commit()\n",
    "\n",
    "# Close the session\n",
    "session.close()\n",
    "\n",
    "print(\"Admin and User roles have been added to the database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bcrypt\n",
      "  Downloading bcrypt-4.2.0-cp39-abi3-win_amd64.whl.metadata (9.9 kB)\n",
      "Downloading bcrypt-4.2.0-cp39-abi3-win_amd64.whl (151 kB)\n",
      "Installing collected packages: bcrypt\n",
      "Successfully installed bcrypt-4.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bcrypt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Admin user updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import bcrypt\n",
    "# Admin user details\n",
    "username = \"Admin\"\n",
    "email = \"admin@hau.edu.ph\"\n",
    "password = \"@ngeliteF0reca$t4dmin\"  # Replace with the actual password you want to use\n",
    "\n",
    "# Hash the password\n",
    "hashed_password = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt(rounds=10))\n",
    "\n",
    "try:\n",
    "    # Check if the user already exists\n",
    "    existing_user = session.execute(text(\"SELECT * FROM users WHERE username = :username OR email = :email\"),\n",
    "                                    {\"username\": username, \"email\": email}).fetchone()\n",
    "    \n",
    "    if existing_user:\n",
    "        # Update the existing user\n",
    "        session.execute(text(\"\"\"\n",
    "            UPDATE users \n",
    "            SET username = :username, email = :email, password_hash = :password_hash\n",
    "            WHERE id = :user_id\n",
    "        \"\"\"), {\n",
    "            \"username\": username,\n",
    "            \"email\": email,\n",
    "            \"password_hash\": hashed_password.decode('utf-8'),\n",
    "            \"user_id\": existing_user[0]\n",
    "        })\n",
    "        print(\"Admin user updated successfully.\")\n",
    "    else:\n",
    "        # Insert the new admin user\n",
    "        session.execute(text(\"\"\"\n",
    "            INSERT INTO users (username, email, password_hash)\n",
    "            VALUES (:username, :email, :password_hash)\n",
    "        \"\"\"), {\n",
    "            \"username\": username,\n",
    "            \"email\": email,\n",
    "            \"password_hash\": hashed_password.decode('utf-8')  # Convert bytes to string\n",
    "        })\n",
    "        \n",
    "        # Get the user id of the newly inserted admin\n",
    "        user_id = session.execute(text(\"SELECT id FROM users WHERE username = :username\"),\n",
    "                                  {\"username\": username}).scalar()\n",
    "        \n",
    "        # Insert the admin role for this user\n",
    "        session.execute(text(\"\"\"\n",
    "            INSERT INTO user_roles (user_id, role_id)\n",
    "            VALUES (:user_id, (SELECT id FROM roles WHERE name = 'Admin'))\n",
    "        \"\"\"), {\n",
    "            \"user_id\": user_id\n",
    "        })\n",
    "        \n",
    "        session.commit()\n",
    "        print(\"Admin user added successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    session.rollback()\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    session.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Major Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, DateTime\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.sql import func\n",
    "\n",
    "\n",
    "# Create a base class for declarative models\n",
    "Base = declarative_base()\n",
    "\n",
    "# Define the Major model\n",
    "class Major(Base):\n",
    "    __tablename__ = 'majors'\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String(100), nullable=False, unique=True)\n",
    "    department = Column(String(50), nullable=False)\n",
    "    latest_year = Column(Integer)  # New column for the latest year\n",
    "    created_at = Column(DateTime(timezone=True), server_default=func.now())\n",
    "    updated_at = Column(DateTime(timezone=True), onupdate=func.now())\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<Major(name='{self.name}', department='{self.department}', latest_year={self.latest_year})>\"\n",
    "\n",
    "# Create the table in the database\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "# Create a session to interact with the database\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# # Example: Add a new major\n",
    "# new_major = Major(name=\"Computer Science\", department=\"School of Computing\")\n",
    "# session.add(new_major)\n",
    "# session.commit()\n",
    "\n",
    "# # Example: Query all majors\n",
    "# majors = session.query(Major).all()\n",
    "# for major in majors:\n",
    "#     print(major)\n",
    "\n",
    "# # Don't forget to close the session when you're done\n",
    "# session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Major Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Major' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m unique_majors \u001b[38;5;241m=\u001b[39m college_data\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMajor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDepartment\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart_Year\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Get the list of existing majors from the database\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m existing_majors \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mquery(\u001b[43mMajor\u001b[49m\u001b[38;5;241m.\u001b[39mname, Major\u001b[38;5;241m.\u001b[39mlatest_year)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m     44\u001b[0m existing_majors_dict \u001b[38;5;241m=\u001b[39m {major[\u001b[38;5;241m0\u001b[39m]: major[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m major \u001b[38;5;129;01min\u001b[39;00m existing_majors}\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Add or update each unique Major-Department combination in the majors table\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Major' is not defined"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, DateTime\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.sql import func\n",
    "import pandas as pd\n",
    "\n",
    "# Create a session\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "def get_college_data(df):\n",
    "    df = df[~df['Department'].isin(['GS', 'JHS', 'SHS', 'HAUSPELL', 'HAU', 'MA'])]\n",
    "    df = df[~df['Major'].isin(['TOTAL', 'GRAND TOTAL'])]\n",
    "    df = df.drop(df.loc[:, \"Grade_1\":\"Grade_12\"].columns, axis=1)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def clean_college_data(df):\n",
    "    # Step 1: Replace similar majors\n",
    "    for old_majors, new_major in similar_majors_dict.items():\n",
    "        df['Major'] = df['Major'].replace(old_majors, new_major[0])\n",
    "     \n",
    "    # Step 4: Drop unwanted majors\n",
    "    df = df[~df['Major'].isin(drop_majors)]\n",
    "        \n",
    "    # Step 6: Combine majors with the same name within the same semester\n",
    "    df = df.groupby(['Start_Year', 'Semester',\n",
    "                     'Major', 'Department'], as_index=False).sum([\"1st_Year\", \"2nd_Year\", \"3rd_Year\",\n",
    "                                                                  \"4th_Year\", \"5th_Year\", \"TOTAL\"])\n",
    "    return df\n",
    "\n",
    "# Load the enrollment data\n",
    "enrollment_df = pd.read_sql_table('enrollment', engine)\n",
    "\n",
    "# Apply the get_college_data function\n",
    "college_data = get_college_data(enrollment_df)\n",
    "college_data = clean_college_data(college_data)\n",
    "\n",
    "# Get unique combinations of Major and Department with the latest year\n",
    "unique_majors = college_data.groupby(['Major', 'Department'])['Start_Year'].max().reset_index()\n",
    "\n",
    "# Get the list of existing majors from the database\n",
    "existing_majors = session.query(Major.name, Major.latest_year).all()\n",
    "existing_majors_dict = {major[0]: major[1] for major in existing_majors}\n",
    "\n",
    "# Add or update each unique Major-Department combination in the majors table\n",
    "new_majors = []\n",
    "updated_majors = []\n",
    "for _, row in unique_majors.iterrows():\n",
    "    if row['Major'] not in existing_majors_dict:\n",
    "        new_majors.append(Major(name=row['Major'], department=row['Department'], latest_year=row['Start_Year']))\n",
    "        print(f\"Adding new major: {row['Major']} - {row['Department']} (Year: {row['Start_Year']})\")\n",
    "    elif row['Start_Year'] > existing_majors_dict[row['Major']]:\n",
    "        major = session.query(Major).filter_by(name=row['Major']).first()\n",
    "        major.latest_year = row['Start_Year']\n",
    "        updated_majors.append(major)\n",
    "        print(f\"Updating major: {row['Major']} - New latest year: {row['Start_Year']}\")\n",
    "    else:\n",
    "        print(f\"Major already up to date: {row['Major']} (Year: {existing_majors_dict[row['Major']]})\")\n",
    "\n",
    "# Bulk insert new majors and update existing ones\n",
    "if new_majors:\n",
    "    session.bulk_save_objects(new_majors)\n",
    "    print(f\"Added {len(new_majors)} new majors to the database.\")\n",
    "if updated_majors:\n",
    "    session.bulk_save_objects(updated_majors, update_changed_only=True)\n",
    "    print(f\"Updated {len(updated_majors)} existing majors in the database.\")\n",
    "\n",
    "# Commit the changes\n",
    "session.commit()\n",
    "\n",
    "# Close the session\n",
    "session.close()\n",
    "\n",
    "print(\"Major update process completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Get the current year\n",
    "current_year = datetime.datetime.now().year\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"data/Enrollment_Data.csv\")\n",
    "cpi_df = pd.read_csv(\"data/CPI_Education.csv\")\n",
    "inflation_df = pd.read_csv(\"data/Inflation_Rate.csv\")\n",
    "admission_df = pd.read_csv(\"data/Admission_Data.csv\")\n",
    "hfce_df = pd.read_csv(\"data/HFCE.csv\")\n",
    "\n",
    "df = clean_data(df)\n",
    "# Basic preprocessing\n",
    "df = df.drop(columns=[\"End_Year\"])\n",
    "\n",
    "# Function to add placeholder for current year\n",
    "def add_current_year_placeholder(df, year_column, value_columns):\n",
    "    max_year = df[year_column].max()\n",
    "    if max_year < current_year:\n",
    "        placeholder = df[df[year_column] == max_year].copy()\n",
    "        placeholder[year_column] = current_year\n",
    "        for col in value_columns:\n",
    "            placeholder[col] = float('nan')\n",
    "        df = pd.concat([df, placeholder], ignore_index=True)\n",
    "    return df.sort_values(by=[year_column]).reset_index(drop=True)\n",
    "\n",
    "# CPI data processing\n",
    "cpi_df_copy = cpi_df.copy()\n",
    "cpi_df_copy[\"Month\"] = cpi_df[\"Month\"].map({\n",
    "    \"Jan\": 1, \"Feb\": 2, \"Mar\": 3, \"Apr\": 4, \"May\": 5, \"Jun\": 6,\n",
    "    \"Jul\": 7, \"Aug\": 8, \"Sep\": 9, \"Oct\": 10, \"Nov\": 11, \"Dec\": 12,\n",
    "})\n",
    "cpi_df_copy = cpi_df_copy.dropna()\n",
    "cpi_df_copy[\"Year\"] = cpi_df_copy[\"Year\"].astype(int)\n",
    "cpi_df_copy = cpi_df_copy.groupby('Year')['CPI_Region3'].mean().reset_index()\n",
    "cpi_df_copy = add_current_year_placeholder(cpi_df_copy, \"Year\", [\"CPI_Region3\"])\n",
    "\n",
    "# Inflation data processing\n",
    "inflation_df = inflation_df[[\"Start_Year\", \"Inflation_Rate\"]].dropna()\n",
    "\n",
    "inflation_df = add_current_year_placeholder(inflation_df, \"Start_Year\", [\"Inflation_Rate\"])\n",
    "inflation_df = create_lag_features(inflation_df, group=None, target=\"Inflation_Rate\", lag_steps=1)\n",
    "inflation_df = inflation_df.drop(columns=[\"Inflation_Rate\"])\n",
    "\n",
    "# HFCE data processing\n",
    "hfce_df = hfce_df.groupby('Start_Year').mean().reset_index()\n",
    "hfce_df = add_current_year_placeholder(hfce_df, \"Start_Year\", [\"HFCE_Education\", \"HFCE\"])\n",
    "\n",
    "# Admission data processing\n",
    "admission_df = admission_df.drop(columns=[\"Number_of_Processed_Applicants\", \"Number_of_Enrolled_Applicants\"])\n",
    "admission_df[\"Department\"] = admission_df[\"Department\"].replace(\"CICT\", \"SOC\")\n",
    "admission_df = add_current_year_placeholder(admission_df, \"Start_Year\", [\"Number_of_Applicants\"])\n",
    "\n",
    "# Basic enrollment data processing\n",
    "df = df.groupby(['Start_Year', 'Semester', 'Major', 'Department'], as_index=False).sum([\"1st_Year\", \"2nd_Year\", \"3rd_Year\", \"4th_Year\", \"5th_Year\", \"Grade_12\", \"TOTAL\"])\n",
    "df = df[~df['Department'].isin(['GS', 'JHS', 'SHS', 'HAUSPELL', 'HAU', 'MA'])]\n",
    "df = df[~df['Major'].isin(['TOTAL', 'GRAND TOTAL'])]\n",
    "df = df.drop(df.loc[:, \"Grade_1\":\"Grade_11\"].columns, axis=1)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Apply the function to determine start month\n",
    "df['Start_Month'] = df['Semester'].apply(determine_start_month)\n",
    "\n",
    "# Convert numerical features to int\n",
    "numerical_features = df.drop(columns=[\"Major\", \"Department\"]).columns\n",
    "df[numerical_features] = df[numerical_features].astype(int)\n",
    "\n",
    "# Merging data\n",
    "df_encoded = df.copy()\n",
    "df_encoded = df_encoded.merge(cpi_df_copy, left_on=[\"Start_Year\"], right_on=[\"Year\"], how=\"left\")\n",
    "df_encoded = df_encoded.merge(inflation_df, on=[\"Start_Year\"], how=\"left\")\n",
    "df_encoded = df_encoded.merge(admission_df, on=[\"Department\", \"Start_Year\"], how=\"left\")\n",
    "df_encoded = df_encoded.merge(hfce_df, on=[\"Start_Year\"], how=\"left\")\n",
    "df_encoded = df_encoded.drop(columns=[\"Quarter\", \"Year\"])\n",
    "\n",
    "# Sort the DataFrame\n",
    "df_encoded = df_encoded.sort_values(by=['Start_Year', 'Start_Month'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Start_Year', 'Semester', 'Major', 'Department', '1st_Year', '2nd_Year',\n",
       "       '3rd_Year', '4th_Year', '5th_Year', 'Grade_12', 'TOTAL', 'Start_Month',\n",
       "       'CPI_Region3', 'Inflation_Rate_lag_1', 'Number_of_Applicants',\n",
       "       'HFCE_Education', 'HFCE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data has been saved to the 'processed_data' table in the database.\n"
     ]
    }
   ],
   "source": [
    "# Create the table and insert the data\n",
    "df_encoded.to_sql('processed_factors', engine, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Processed data has been saved to the 'processed_data' table in the database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data/Enrollment_Data.csv\")\n",
    "cpi_df = pd.read_csv(\"data/CPI_Education.csv\")\n",
    "inflation_df = pd.read_csv(\"data/Inflation_Rate.csv\")\n",
    "admission_df = pd.read_csv(\"data/Admission_Data.csv\")\n",
    "hfce_df = pd.read_csv(\"data/HFCE.csv\")\n",
    "\n",
    "df = df.drop(columns=[\"End_Year\"])\n",
    "\n",
    "admission_df = admission_df.drop(columns=[\"Number_of_Processed_Applicants\", \"Number_of_Enrolled_Applicants\"])\n",
    "admission_df[\"Department\"] = admission_df[\"Department\"].replace(\"CICT\", \"SOC\")\n",
    "\n",
    "# Get Exponential Moving Average for Admission Number of Applicants\n",
    "# Filter the 2023 data\n",
    "admission_2023 = admission_df[admission_df[\"Start_Year\"] == 2023].copy()\n",
    "\n",
    "# Update the Start_Year to 2024\n",
    "admission_2023[\"Start_Year\"] = 2024\n",
    "\n",
    "# Set the Number_of_Applicants values to NaN for the 2024 data\n",
    "admission_2023[\"Number_of_Applicants\"] = float('nan')\n",
    "\n",
    "# Append the new 2024 data to the original admission_df\n",
    "admission_df = pd.concat([admission_df, admission_2023], ignore_index=True)\n",
    "\n",
    "# Ensure the DataFrame is sorted by 'Start_Year' and reset the index\n",
    "admission_df = admission_df.sort_values(by=['Start_Year']).reset_index(drop=True)\n",
    "\n",
    "admission_df = admission_df[admission_df[\"Start_Year\"] <= 2024]\n",
    "admission_df = create_lag_features(admission_df, group=[\"Department\"], target=\"Number_of_Applicants\", lag_steps=1)\n",
    "# admission_df = create_rolling_mean(admission_df, group=[\"Department\"], target=\"Number_of_Applicants\", window_size=3, min_periods=1)\n",
    "admission_df = create_rolling_std(admission_df, group=[\"Department\"], target=\"Number_of_Applicants\", window_size=3, min_periods=1, lag_steps=0)\n",
    "# admission_df = create_lag_features(admission_df, group=[\"Department\"], target=\"Number_of_Applicants\", window_size=3, min_periods=1, lag_steps=1)\n",
    "\n",
    "admission_df = admission_df.fillna(0)\n",
    "\n",
    "cpi_df_copy = cpi_df.copy()\n",
    "# cpi_df_copy = cpi_df_copy[cpi_df_copy[\"Year\"] < 2024]\n",
    "cpi_df_copy[\"Month\"] = cpi_df[\"Month\"].map({\n",
    "    \"Jan\": 1,\n",
    "    \"Feb\": 2,\n",
    "    \"Mar\": 3,\n",
    "    \"Apr\": 4,\n",
    "    \"May\": 5,\n",
    "    \"Jun\": 6,\n",
    "    \"Jul\": 7,\n",
    "    \"Aug\": 8,\n",
    "    \"Sep\": 9,\n",
    "    \"Oct\": 10,\n",
    "    \"Nov\": 11,\n",
    "    \"Dec\": 12,\n",
    "})\n",
    "cpi_df_copy = cpi_df_copy.dropna()\n",
    "cpi_df_copy[[\"Year\"]] = cpi_df_copy[[\"Year\"]].astype(int)\n",
    "cpi_df_copy = cpi_df_copy.groupby('Year')['CPI_Region3'].mean().reset_index()\n",
    "cpi_df_copy = create_rolling_std(cpi_df_copy, group=None, target=\"CPI_Region3\", window_size=6, lag_steps=0)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Step 1: Replace similar majors\n",
    "for old_majors, new_major  in similar_majors_dict.items():\n",
    "    df['Major'] = df['Major'].replace(old_majors, new_major[0])\n",
    "\n",
    "# Step 2: Correct departments\n",
    "\n",
    "for major, department in incorrect_department_dict.items():\n",
    "    df.loc[df[\"Major\"]== major, 'Department'] = department\n",
    "    \n",
    "# Step 3: Rename departments\n",
    "df['Department'] = df['Department'].replace(department_dict)\n",
    "\n",
    "# Step 4: Drop unwanted majors\n",
    "df = df[~df['Major'].isin(drop_majors)]\n",
    "\n",
    "# # Step 5: Filter by threshold\n",
    "# major_counts = df['Major'].value_counts()\n",
    "# valid_majors = major_counts[major_counts >= threshold].index\n",
    "# df = df[finalDf['Major'].isin(valid_majors)]\n",
    "\n",
    "# Step 6: Combine majors with the same name within the same semester\n",
    "df = df.groupby(['Start_Year', 'Semester', 'Major', 'Department'], as_index=False).sum([\"1st_Year\", \"2nd_Year\", \"3rd_Year\", \"4th_Year\", \"5th_Year\", \"Grade_12\", \"TOTAL\"])\n",
    "\n",
    "\n",
    "shs_df = df[df[\"Department\"] == \"SHS\"].pivot_table(index=[\"Start_Year\", \"Semester\"], columns=\"Major\", values=\"Grade_12\").reset_index().fillna(0)\n",
    "shs_df[\"Start_Year\"] += 1\n",
    "shs_df = shs_df.rename(columns={col: f\"{col}_lag_1\" for col in shs_df.columns.drop([\"Start_Year\", \"Semester\"])})\n",
    "\n",
    "\n",
    "\n",
    "df = df[~df['Department'].isin(['GS', 'JHS', 'SHS', 'HAUSPELL', 'HAU', 'MA'])]\n",
    "df = df[~df['Major'].isin(['TOTAL', 'GRAND TOTAL'])]\n",
    "df = df.drop(df.loc[:, \"Grade_1\":\"Grade_11\"].columns, axis=1)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inflation_df = inflation_df[[\"Start_Year\", \"Inflation_Rate\"]]\n",
    "inflation_df_copy = inflation_df.dropna()\n",
    "inflation_2023 = inflation_df_copy[inflation_df_copy[\"Start_Year\"] == 2023].copy()\n",
    "\n",
    "# Update the Start_Year to 2024\n",
    "inflation_2023[\"Start_Year\"] = 2024\n",
    "\n",
    "# Set the Number_of_Applicants values to NaN for the 2024 data\n",
    "inflation_2023[\"Inflation_Rate\"] = float('nan')\n",
    "\n",
    "# Append the new 2024 data to the original admission_df\n",
    "inflation_df_copy = pd.concat([inflation_df_copy, inflation_2023], ignore_index=True)\n",
    "\n",
    "# Ensure the DataFrame is sorted by 'Start_Year' and reset the index\n",
    "inflation_df_copy = inflation_df_copy.sort_values(by=['Start_Year']).reset_index(drop=True)\n",
    "\n",
    "inflation_df_copy = inflation_df_copy[inflation_df_copy[\"Start_Year\"] <= 2024]\n",
    "\n",
    "# inflation_df_copy = create_lag_features(cpi_df_copy, group=None, target=\"Inflation_Rate\", lag_steps=1)\n",
    "# inflation_df_copy = create_rolling_mean(cpi_df_copy, group=None, target=\"Inflation_Rate\", window_size=6)\n",
    "inflation_df_copy = create_rolling_std(inflation_df_copy, group=None, target=\"Inflation_Rate\", window_size=3, lag_steps=1)\n",
    "inflation_df_copy = inflation_df_copy.drop(columns=[\"Inflation_Rate\"])\n",
    "\n",
    "\n",
    "# HFCE data processing\n",
    "hfce_df = hfce_df.groupby('Start_Year').mean().reset_index()\n",
    "hfce_df_copy = hfce_df.dropna()\n",
    "hfce_df_copy = create_lag_features(hfce_df_copy, group=None, target=\"HFCE_Education\", lag_steps=1)\n",
    "hfce_df_copy = create_rolling_std(hfce_df_copy, group=None, target=\"HFCE_Education\", window_size=6)\n",
    "hfce_df_copy = create_lag_features(hfce_df_copy, group=None, target=\"HFCE\", lag_steps=1)\n",
    "hfce_df_copy = create_rolling_std(hfce_df_copy, group=None, target=\"HFCE\", window_size=6)\n",
    "hfce_df_copy = hfce_df_copy.drop(columns=[\"HFCE_Education\", \"HFCE\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Apply the function and create Semester_Start column\n",
    "df['Start_Month'] = df['Semester'].apply(determine_start_month)\n",
    "\n",
    "# Get End Month after 18 weeks\n",
    "# df['End_Month'] = df['Start_Month'] + 4\n",
    "df = df.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# # Add 'Modified' column where 1st_Year is 0\n",
    "# df['Modified'] = df['1st_Year'].apply(lambda x: True if x == 0 else False)\n",
    "\n",
    "\n",
    "numerical_features =  df.drop(columns=[\"Major\", \"Department\"]).columns\n",
    "df[numerical_features] = df[numerical_features].astype(int)\n",
    "\n",
    "\n",
    "# Add CPI_Region3 to the dataframe\n",
    "df_encoded = df.copy()\n",
    "df_encoded[\"Start_Year\"].unique()\n",
    "\n",
    "\n",
    "# df_encoded = df.copy()\n",
    "# df_encoded.drop(drop_majors, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Merging data\n",
    "df_encoded = df_encoded.merge(cpi_df_copy, left_on=[\"Start_Year\"], right_on=[\"Year\"], how=\"left\")\n",
    "df_encoded = df_encoded.merge(inflation_df_copy, on=[\"Start_Year\"], how=\"left\")\n",
    "df_encoded = df_encoded.merge(admission_df, on=[\"Department\", \"Start_Year\"], how=\"left\")\n",
    "df_encoded = df_encoded.merge(hfce_df_copy, on=[\"Start_Year\"], how=\"left\")\n",
    "df_encoded = df_encoded.drop(columns=[\"Year\"])\n",
    "\n",
    "\n",
    "# Lagged Features\n",
    "# Calculate Exponential Moving Average (EMA)\n",
    "ema_df = pd.DataFrame()\n",
    "# ema_df['Inflation_Rate_EMA'] = df_encoded.groupby(['Start_Year', 'Major'])['Inflation_Rate'].transform(lambda x: x.ewm(span=2, adjust=False).mean())\n",
    "\n",
    "# Ensure DataFrame is sorted by 'Start_Year' and index\n",
    "df_encoded.sort_values(by=['Start_Year', 'Semester'], inplace=True)\n",
    "\n",
    "df_encoded = create_lag_features(df_encoded, lag_steps=1)\n",
    "# df_encoded = create_rolling_mean(df_encoded, lag_steps=1, window_size=3)\n",
    "df_encoded = create_rolling_std(df_encoded, lag_steps=1, window_size=3)\n",
    "\n",
    "# df_encoded = create_lag_features(df_encoded, lag_steps=2, target=\"Grade_12\")\n",
    "# df_encoded = create_rolling_std(df_encoded, lag_steps=2, window_size=3, target=\"Grade_12\")\n",
    "\n",
    "df_encoded = create_lag_features(df_encoded, lag_steps=1, target=\"TOTAL\")\n",
    "\n",
    "# df_encoded = create_rolling_std(df_encoded, window_size=3, target=\"2nd_Year\")\n",
    "# df_encoded = create_rolling_std(df_encoded, window_size=3, target=\"3rd_Year\")\n",
    "# df_encoded = create_rolling_std(df_encoded, window_size=3, target=\"4th_Year\")\n",
    "\n",
    "\n",
    "\n",
    "# df_encoded = df_encoded.drop(columns=[\"Number_of_Applicants\", \"CPI_Region3\"])\n",
    "\n",
    "\n",
    "\n",
    "# ema_df['1st_Year_EMA'] = df_encoded.groupby('Major')['1st_Year'].transform(lambda x: x.ewm(span=2, adjust=False).mean())\n",
    "# ema_df['2nd_Year_EMA'] = df_encoded.groupby('Major')['2nd_Year'].transform(lambda x: x.ewm(span=2, adjust=False).mean())\n",
    "# ema_df['3rd_Year_EMA'] = df_encoded.groupby('Major')['3rd_Year'].transform(lambda x: x.ewm(span=2, adjust=False).mean())\n",
    "# ema_df['4th_Year_EMA'] = df_encoded.groupby('Major')['4th_Year'].transform(lambda x: x.ewm(span=2, adjust=False).mean())\n",
    "\n",
    "#df_encoded = df_encoded.dropna()\n",
    "\n",
    "\n",
    "# Assume df_train is your training dataframe\n",
    "# One-hot encode departments and majors\n",
    "dept_encoder = OneHotEncoder(sparse_output=False)\n",
    "major_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "dept_encoded = dept_encoder.fit_transform(df_encoded[['Department']])\n",
    "major_encoded = major_encoder.fit_transform(df_encoded[['Major']])\n",
    "\n",
    "# Perform PCA\n",
    "dept_pca = PCA(n_components=2, random_state=24)\n",
    "major_pca = PCA(n_components=2, random_state=24)\n",
    "\n",
    "department_principalComponents = dept_pca.fit_transform(dept_encoded)\n",
    "department_principalDf = pd.DataFrame(data = department_principalComponents, columns = [f\"Department_PC{i+1}\" for i in range(2)])\n",
    "\n",
    "major_principalComponents = major_pca.fit_transform(major_encoded)\n",
    "major_principalDf = pd.DataFrame(data = major_principalComponents, columns = [f\"Major_PC{i+1}\" for i in range(2)])\n",
    "\n",
    "\n",
    "# Save the encoders and PCA objects\n",
    "joblib.dump(dept_encoder, 'data/dept_encoder.pkl')\n",
    "joblib.dump(major_encoder, 'data/major_encoder.pkl')\n",
    "joblib.dump(dept_pca, 'data/dept_pca.pkl')\n",
    "joblib.dump(major_pca, 'data/major_pca.pkl')\n",
    "\n",
    "\n",
    "# department_df = pd.get_dummies(df_encoded[[\"Department\"]])\n",
    "# major_df = pd.get_dummies(df_encoded[[\"Major\"]])\n",
    "# # df_encoded = pd.concat([df_encoded, major_df], axis=1)\n",
    "\n",
    "\n",
    "# # Dimensionality Reduction for Major\n",
    "# pca = PCA(n_components=2, random_state=24)\n",
    "# department_principalComponents = pca.fit_transform(department_df)\n",
    "# department_principalDf = pd.DataFrame(data = department_principalComponents, columns = [f\"Department_PC{i+1}\" for i in range(2)])\n",
    "\n",
    "# major_principalComponents = pca.fit_transform(major_df)\n",
    "# major_principalDf = pd.DataFrame(data = major_principalComponents, columns = [f\"Major_PC{i+1}\" for i in range(2)])\n",
    "\n",
    "\n",
    "finalDf = pd.concat([df_encoded, department_principalDf, major_principalDf], axis = 1)\n",
    "\n",
    "# finalDf = finalDf.dropna()\n",
    "\n",
    "# Drop major with less than 10 students overall\n",
    "major_counts = finalDf.groupby('Major')['1st_Year'].sum()\n",
    "valid_majors = major_counts[major_counts >= 10].index\n",
    "finalDf = finalDf[finalDf['Major'].isin(valid_majors)]\n",
    "\n",
    "finalDf = finalDf.merge(shs_df, on=[\"Start_Year\", \"Semester\"], how=\"left\")\n",
    "finalDf = finalDf.drop(columns=[\"CPI_Region3\", \"Number_of_Applicants\"])\n",
    "\n",
    "\n",
    "# Step 1: Group by year and major to get the sum of students in each major for each year\n",
    "grouped = finalDf.groupby(['Start_Year', 'Semester', 'Major'])['1st_Year_lag_1'].sum().reset_index()\n",
    "\n",
    "# Step 2: Calculate the total number of students for each year\n",
    "total_students_per_year = grouped.groupby(['Start_Year', 'Semester'])['1st_Year_lag_1'].sum().reset_index()\n",
    "total_students_per_year.rename(columns={'1st_Year_lag_1': 'Total_1st_Year_Students_lag_1'}, inplace=True)\n",
    "\n",
    "# Step 3: Merge the total students per year with the grouped data\n",
    "distribution_df = pd.merge(grouped, total_students_per_year, on=['Start_Year', 'Semester'])\n",
    "\n",
    "# Step 4: Calculate the percentage distribution of each major\n",
    "distribution_df['Percentage_Distribution_lag_1'] = (distribution_df['1st_Year_lag_1'] / distribution_df['Total_1st_Year_Students_lag_1']) * 100\n",
    "\n",
    "\n",
    "finalDf = finalDf.merge(distribution_df.drop(columns=[\"1st_Year_lag_1\"]), on=['Start_Year', 'Semester', 'Major'])\n",
    "\n",
    "\n",
    "# # Create multiple columns filled with 0s\n",
    "# for i in range(3, 5):\n",
    "#     finalDf[f\"Grade_12_lag_{i}\"] = 1\n",
    "    \n",
    "\n",
    "finalDf = finalDf.sort_values(by=['Start_Year', 'Start_Month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "747"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDf_copy = finalDf.fillna(0).drop_duplicates()\n",
    "finalDf_copy.to_sql('processed_data', engine, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAUTION\n",
    "DO NOT RUN THIS CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throw error to avoid run all to here\n",
    "raise Exception(\"Execution stopped to prevent running the irreversible code.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear majors Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sessionmaker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create a new session\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m Session \u001b[38;5;241m=\u001b[39m \u001b[43msessionmaker\u001b[49m(bind\u001b[38;5;241m=\u001b[39mengine)\n\u001b[0;32m      5\u001b[0m session \u001b[38;5;241m=\u001b[39m Session()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Delete all rows from the majors table\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sessionmaker' is not defined"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "# Create a new session\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "try:\n",
    "    # Delete all rows from the majors table\n",
    "    session.execute(text(\"DELETE FROM majors\"))\n",
    "    \n",
    "    # Commit the transaction\n",
    "    session.commit()\n",
    "    print(\"All rows have been deleted from the majors table.\")\n",
    "except Exception as e:\n",
    "    # If an error occurs, rollback the changes\n",
    "    session.rollback()\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the session\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete majors Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'majors' table has been successfully dropped.\n",
      "Table deletion process completed.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, MetaData\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# Create a MetaData instance\n",
    "metadata = MetaData()\n",
    "\n",
    "# Reflect the existing tables\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "\n",
    "try:\n",
    "    # Check if the majors table exists\n",
    "    if 'majors' in metadata.tables:\n",
    "        # Drop the majors table\n",
    "        metadata.tables['majors'].drop(engine)\n",
    "        print(\"The 'majors' table has been successfully dropped.\")\n",
    "    else:\n",
    "        print(\"The 'majors' table does not exist.\")\n",
    "    \n",
    "    # Commit the transaction\n",
    "    session.commit()\n",
    "except Exception as e:\n",
    "    # If an error occurs, rollback the changes\n",
    "    session.rollback()\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the session\n",
    "    session.close()\n",
    "\n",
    "print(\"Table deletion process completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete processed_data Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'processed_data' table does not exist.\n",
      "Table deletion process completed.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# Create a MetaData instance\n",
    "metadata = MetaData()\n",
    "\n",
    "# Reflect the existing tables\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "# Create a session\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "try:\n",
    "    # Check if the processed_data table exists\n",
    "    if 'processed_data' in metadata.tables:\n",
    "        # Drop the processed_data table\n",
    "        metadata.tables['processed_data'].drop(engine)\n",
    "        print(\"The 'processed_data' table has been successfully dropped.\")\n",
    "    else:\n",
    "        print(\"The 'processed_data' table does not exist.\")\n",
    "    \n",
    "    # Commit the transaction\n",
    "    session.commit()\n",
    "except Exception as e:\n",
    "    # If an error occurs, rollback the changes\n",
    "    session.rollback()\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the session\n",
    "    session.close()\n",
    "\n",
    "print(\"Table deletion process completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enrollment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
