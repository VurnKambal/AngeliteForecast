{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install psycopg2\n",
    "%pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateDatabase",
     "evalue": "database \"angeliteforecast\" already exists\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDuplicateDatabase\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     conn\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Create the database\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[43mcreate_database\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mangeliteforecast\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpostgres\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthesis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m, in \u001b[0;36mcreate_database\u001b[1;34m(db_name, username, password, host, port)\u001b[0m\n\u001b[0;32m      7\u001b[0m conn\u001b[38;5;241m.\u001b[39mset_isolation_level(psycopg2\u001b[38;5;241m.\u001b[39mextensions\u001b[38;5;241m.\u001b[39mISOLATION_LEVEL_AUTOCOMMIT)\n\u001b[0;32m      8\u001b[0m cur \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[1;32m----> 9\u001b[0m \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCREATE DATABASE \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdb_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m cur\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     11\u001b[0m conn\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mDuplicateDatabase\u001b[0m: database \"angeliteforecast\" already exists\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Function to create a new database\n",
    "def create_database(db_name, username, password, host='localhost', port=5432):\n",
    "    conn = psycopg2.connect(dbname=\"postgres\", user=username, password=password, host=host, port=port)\n",
    "    conn.set_isolation_level(psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"CREATE DATABASE {db_name};\")\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "# Create the database\n",
    "create_database('angeliteforecast', 'postgres', 'thesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "def connect_to_database(username, password, host='localhost', port=5432, db_name='angeliteforecast'):\n",
    "    \"\"\"Create a database connection.\"\"\"\n",
    "    connection_string = f'postgresql://{username}:{password}@{host}:{port}/{db_name}'\n",
    "    engine = create_engine(connection_string)\n",
    "    return engine\n",
    "engine = connect_to_database(\"postgres\", \"thesis\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "similar_majors_dict = {\n",
    "    # CCJEF\n",
    "    \"Criminology\": [\"BS Criminology\"], # Combine Criminology and BS Criminology\n",
    "\n",
    "    \n",
    "    # SAS\n",
    "    \"ABComm\": [\"BA Comm\"],\n",
    "    \"CommArts\": [\"BA Comm\"],\n",
    "    \"AdvPublicRel\": [\"BA Comm\"], # Area of Specialization\n",
    "    # \"CommArts\": \"BA Comm\",     # Lack of Data | Drop\n",
    "\n",
    "\n",
    "    # SBA\n",
    "    \"Accounting\": [\"Accountancy\"],\n",
    "    \"Accounting Tech\": [\"BSAcctgInfoSys\"],\n",
    "    \"BSBA-BM-Marketi\": [\"BusMgt\", \"BSBA-MktgMgmt\"],\n",
    "    \"BussMgmt-HRM\": [\"BusMgt\", \"BSBA-HRMgmt\"],\n",
    "\n",
    "\n",
    "    # SEA\n",
    "    \"ECE\": [\"ELECENG\"],\n",
    "    # \"BSCE-CEM\": [\"CE\", \"BSCE-CEM\"],\n",
    "    # \"BSCE-SE\": [\"CE\", \"BSCE-SE\"],\n",
    "    # \"BSCE-TE\": [\"CE\", \"BSCE-TE\"],\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # SED\n",
    "    \"BEED-MajSPED\": [\"BSNEd\"],           # Name change?\n",
    "    \"BEED-SpecialEdu\": [\"BSNEd\"],        # Name change?\n",
    "    \"BPE-SPE\": [\"BPEd\"],                   # Name change?\n",
    "    \"BSED-ValEd\": [\"BSED-RelValEd\"],\n",
    "    \"BSMath\": [\"Math\"],\n",
    "    \n",
    "\n",
    "    # SHTM\n",
    "    \"BS EventMgmt\": [\"BSTM-Events\"],\n",
    "\n",
    "\n",
    "    # SOC\n",
    "    \"BSCSSysDev\": [\"BSCompsci\"],              # New Curriculum\n",
    "    \"BSITAnimation\": [\"BSEMC-DA\"],            # New Curriculum\n",
    "    \"BSITAreaAnimati\": [\"BSEMC-DA\"],          # New Curriculum\n",
    "    \"BSCyberplusPSM\": [\"BSCybersecurity\"],    # Same Major but with Professional Science Master's Degree\n",
    "    # \"BSInfoTech\": \"BSIT\",                 # General IT Major | Lack of Data | Drop\n",
    "    # \"BSITMultiTec\": \"BSIT\",               # Discontinued | Lack of Data | Drop\n",
    "    \"BSITAreaNetAdmi\": [\"BSITNetAdmin\"], \n",
    "    \"BSITAreaWebDev\": [\"BSITWebdev\"],\n",
    "\n",
    "    }\n",
    "\n",
    "# Dictionary for major with incorrect department\n",
    "incorrect_department_dict = {\n",
    "    \"BSBA-HRM\": \"SBA\",\n",
    "    # \"MAPEH-BSED\": \"SED\",\n",
    "}\n",
    "\n",
    "department_dict = {\n",
    "    \"CHTM\" : \"SHTM\",\n",
    "    \"CICT\" : \"SOC\",\n",
    "}\n",
    "\n",
    "# List of majors to drop\n",
    "drop_majors = [\n",
    "    # MA\n",
    "    # \"MAPEH-BSED\",     # Lack of data | Discontinued | SED Department?\n",
    "    \"MBM\",              # Lack of data | only 1 student\n",
    "    \"MSEngMgmt\",        # Lack of data | only 1 student\n",
    "    \n",
    "\n",
    "    # SAS\n",
    "    # \"CommArts\",         # Lack of data | only 1 student\n",
    "    \"LanguageLit\",      # Lack of data | only 1 student\n",
    "\n",
    "    # SED\n",
    "    \n",
    "    # SHTM\n",
    "    \"BSBATourism\",\n",
    "    \"BSTM-Tourism\",     # Lack of data | BSTourism? | only 2 students\n",
    "\n",
    "\n",
    "    # SOC\n",
    "    \"AssCompSci\",       # Lack of data | only 1 student\n",
    "    \"BSITMultiTec\",     # Lack of data | Discontinued\n",
    "    \"BSInfoTech\",       # Lack of data | only 1 student\n",
    "\n",
    "    # SEA\n",
    "    \"ECETech.\",        # Lack of data | Discontinued\n",
    "\n",
    "\n",
    "    # Extras | Aggregate Counts\n",
    "    \"TOTAL\",\n",
    "    \"GRAND TOTAL\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \n",
    "\n",
    "    # Step 2: Correct departments\n",
    "\n",
    "    for major, department in incorrect_department_dict.items():\n",
    "        df.loc[df[\"Major\"]== major, 'Department'] = department\n",
    "        \n",
    "    # Step 3: Rename departments\n",
    "    df['Department'] = df['Department'].replace(department_dict)\n",
    "\n",
    "    # Step 4: Drop unwanted majors\n",
    "    # df = df[~df['Major'].isin(drop_majors)]\n",
    "\n",
    "    # df = df[~df['Department'].isin(['GS', 'JHS', 'HAUSPELL', 'HAU', 'MA'])]\n",
    "    df = df.reset_index(drop=True)\n",
    "    df =df.sort_values(by=[\"Start_Year\", \"Semester\", \"Department\", \"Major\"])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/Enrollment_Data.csv\").sort_values(by=[\"Start_Year\", \"Semester\", \"Department\", \"Major\"])\n",
    "cpi_df = pd.read_csv(\"data/CPI_Education.csv\").sort_values(by=[\"Year\", \"Month\"])\n",
    "inflation_df = pd.read_csv(\"data/Inflation_Rate.csv\").sort_values(by=\"Start_Year\")\n",
    "admission_df = pd.read_csv(\"data/Admission_Data.csv\").sort_values(by=[\"Start_Year\", \"Department\"])\n",
    "admission_df[\"Department\"] = admission_df[\"Department\"].replace(\"CICT\", \"SOC\")\n",
    "\n",
    "hfce_df = pd.read_csv(\"data/HFCE.csv\").sort_values(by=[\"Start_Year\", \"Quarter\"])\n",
    "\n",
    "df = clean_data(df)\n",
    "\n",
    "# Upload data to database\n",
    "df.to_sql('enrollment', engine, if_exists='replace', index=False)\n",
    "cpi_df.to_sql('cpi_education', engine, if_exists='replace', index=False)\n",
    "inflation_df.to_sql('inflation_rate', engine, if_exists='replace', index=False)\n",
    "admission_df.to_sql('admission', engine, if_exists='replace', index=False)\n",
    "hfce_df.to_sql('hfce', engine, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "def create_lag_features(data, lag_steps=1, group=[\"Major\"], target=\"1st_Year\"):\n",
    "    if group:\n",
    "        for i in range(1, lag_steps + 1):\n",
    "            data[f'{target}_lag_{i}'] = data.groupby(group)[target].shift(i)\n",
    "    else:\n",
    "        for i in range(1, lag_steps + 1):\n",
    "            data[f'{target}_lag_{i}'] = data[target].shift(i)\n",
    "    return data\n",
    "\n",
    "def create_rolling_mean(data, group=[\"Major\"], window_size=3, target=\"1st_Year\", min_periods=1, lag_steps=0):\n",
    "    lag_string = f'_lag_{lag_steps}' if lag_steps else \"\"\n",
    "    if group:\n",
    "        data[f'{target}_rolling_mean{lag_string}'] = data.groupby(group)[target].shift(lag_steps).rolling(window=window_size, min_periods=min_periods).mean().values\n",
    "    else:\n",
    "        data[f'{target}_rolling_mean{lag_string}'] = data[target].shift(lag_steps).rolling(window=window_size, min_periods=min_periods).mean()\n",
    "    return data\n",
    "\n",
    "def create_rolling_std(data, group=[\"Major\"], window_size=3, target=\"1st_Year\", min_periods=1, lag_steps=0):\n",
    "    lag_string = f'_lag_{lag_steps}' if lag_steps else \"\"\n",
    "    if group:\n",
    "        data[f'{target}_rolling_std{lag_string}'] = data.groupby(group)[target].shift(lag_steps).rolling(window=window_size, min_periods=min_periods).std().values\n",
    "    else:\n",
    "        data[f'{target}_rolling_std{lag_string}'] = data[target].shift(lag_steps).rolling(window=window_size, min_periods=min_periods).std()\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return np.mean(2 * np.abs((y_true - y_pred) /  (np.abs(y_true) + np.abs(y_pred)))) * 100\n",
    "\n",
    "# MAPE\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) /  y_true)) * 100\n",
    "\n",
    "\n",
    "def modelresults(y_true, predictions):\n",
    "    mae = mean_absolute_error(y_true, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, predictions))\n",
    "    r2 = r2_score(y_true, predictions)\n",
    "    smape_score = smape(y_true, predictions)\n",
    "    mape_score = mape(y_true, predictions)\n",
    "\n",
    "    print(f\"Symmetric Mean Absolute Percentage Error: {smape_score:.2f}%\")\n",
    "    print(f'Mean Absolute Percentage Error: {mape_score:.2f}%')\n",
    "    print(f'Mean Absolute Error: {mae:.4f}')\n",
    "    print(f'Root Mean Squared Error: {rmse:.4f}')\n",
    "    print(f'R2 Score: {r2*100:.4f}')\n",
    "\n",
    "    return {\n",
    "        \"smape\": smape_score,\n",
    "        \"mape\": mape_score,\n",
    "        \"mae\": mae,\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\": r2\n",
    "    }\n",
    "\n",
    "\n",
    "# Determine the start of enrollment period\n",
    "def determine_start_month(semester):\n",
    "    if semester == 1:\n",
    "       return 5  # June\n",
    "    else:\n",
    "        \n",
    "        return 10  # November"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables for users, roles, and user_roles have been created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Local\\temp\\ipykernel_9332\\2841564668.py:6: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, ForeignKey, Table\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship\n",
    "from sqlalchemy.sql import func\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "# Define the association table\n",
    "user_roles = Table('user_roles', Base.metadata,\n",
    "    Column('user_id', Integer, ForeignKey('users.id')),\n",
    "    Column('role_id', Integer, ForeignKey('roles.id'))\n",
    ")\n",
    "\n",
    "class User(Base):\n",
    "    __tablename__ = 'users'\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    username = Column(String(50), unique=True, nullable=False)\n",
    "    email = Column(String(100), unique=True, nullable=False)\n",
    "    password_hash = Column(String(255), nullable=False)\n",
    "    created_at = Column(DateTime(timezone=True), server_default=func.now())\n",
    "    updated_at = Column(DateTime(timezone=True), onupdate=func.now())\n",
    "\n",
    "    roles = relationship('Role', secondary=user_roles, back_populates='users')\n",
    "\n",
    "class Role(Base):\n",
    "    __tablename__ = 'roles'\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String(50), unique=True, nullable=False)\n",
    "    description = Column(Text)\n",
    "\n",
    "    users = relationship('User', secondary=user_roles, back_populates='roles')\n",
    "\n",
    "# Create the tables\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "print(\"Tables for users, roles, and user_roles have been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Major Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, DateTime\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.sql import func\n",
    "\n",
    "\n",
    "# Create a base class for declarative models\n",
    "Base = declarative_base()\n",
    "\n",
    "# Define the Major model\n",
    "class Major(Base):\n",
    "    __tablename__ = 'majors'\n",
    "\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String(100), nullable=False, unique=True)\n",
    "    department = Column(String(50), nullable=False)\n",
    "    latest_year = Column(Integer)  # New column for the latest year\n",
    "    created_at = Column(DateTime(timezone=True), server_default=func.now())\n",
    "    updated_at = Column(DateTime(timezone=True), onupdate=func.now())\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<Major(name='{self.name}', department='{self.department}', latest_year={self.latest_year})>\"\n",
    "\n",
    "# Create the table in the database\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "# Create a session to interact with the database\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# # Example: Add a new major\n",
    "# new_major = Major(name=\"Computer Science\", department=\"School of Computing\")\n",
    "# session.add(new_major)\n",
    "# session.commit()\n",
    "\n",
    "# # Example: Query all majors\n",
    "# majors = session.query(Major).all()\n",
    "# for major in majors:\n",
    "#     print(major)\n",
    "\n",
    "# # Don't forget to close the session when you're done\n",
    "# session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Major Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major already up to date: ABPsychology (Year: 2024)\n",
      "Major already up to date: Accountancy (Year: 2024)\n",
      "Major already up to date: Architecture (Year: 2024)\n",
      "Major already up to date: BA Comm (Year: 2024)\n",
      "Major already up to date: BEED-MajSPED  (Year: 2018)\n",
      "Major already up to date: BEEd (Year: 2024)\n",
      "Major already up to date: BForensicSci (Year: 2024)\n",
      "Major already up to date: BPEd (Year: 2024)\n",
      "Major already up to date: BS Criminology (Year: 2024)\n",
      "Major already up to date: BS MedTech (Year: 2024)\n",
      "Major already up to date: BSAcctgInfoSys (Year: 2021)\n",
      "Major already up to date: BSAeronautical (Year: 2024)\n",
      "Major already up to date: BSAviationMgmt (Year: 2024)\n",
      "Major already up to date: BSBA HRDM (Year: 2019)\n",
      "Major already up to date: BSBA-FinMgmt (Year: 2024)\n",
      "Major already up to date: BSBA-HRM (Year: 2019)\n",
      "Major already up to date: BSBA-HRMgmt (Year: 2024)\n",
      "Major already up to date: BSBA-MktgMgmt (Year: 2024)\n",
      "Major already up to date: BSBALegal (Year: 2024)\n",
      "Major already up to date: BSBAMgmtAcctg (Year: 2024)\n",
      "Major already up to date: BSCE-CEM (Year: 2024)\n",
      "Major already up to date: BSCE-SE (Year: 2024)\n",
      "Major already up to date: BSCE-TE (Year: 2024)\n",
      "Major already up to date: BSCompsci (Year: 2024)\n",
      "Major already up to date: BSCulinary (Year: 2024)\n",
      "Major already up to date: BSCybersecurity (Year: 2024)\n",
      "Major already up to date: BSED-RelValEd (Year: 2024)\n",
      "Major already up to date: BSEMC-DA (Year: 2024)\n",
      "Major already up to date: BSHM-Accomm (Year: 2024)\n",
      "Major already up to date: BSHM-Culinary (Year: 2024)\n",
      "Major already up to date: BSHM-RestFood (Year: 2024)\n",
      "Major already up to date: BSHRM (Year: 2023)\n",
      "Major already up to date: BSITNetAdmin (Year: 2024)\n",
      "Major already up to date: BSITWebdev (Year: 2024)\n",
      "Major already up to date: BSIntAuditing (Year: 2024)\n",
      "Major already up to date: BSInterGastro (Year: 2024)\n",
      "Major already up to date: BSNEd (Year: 2024)\n",
      "Major already up to date: BSNursing (Year: 2024)\n",
      "Major already up to date: BSPsychology (Year: 2024)\n",
      "Major already up to date: BSRadTech (Year: 2024)\n",
      "Major already up to date: BSTM-DestMgmt (Year: 2024)\n",
      "Major already up to date: BSTM-Events (Year: 2024)\n",
      "Major already up to date: BSTM-Travel (Year: 2024)\n",
      "Major already up to date: BSTourism (Year: 2024)\n",
      "Major already up to date: Biological (Year: 2019)\n",
      "Major already up to date: BusMgt (Year: 2024)\n",
      "Major already up to date: CE (Year: 2024)\n",
      "Major already up to date: Comp. Eng'g. (Year: 2024)\n",
      "Major already up to date: EE (Year: 2024)\n",
      "Major already up to date: ELECENG (Year: 2024)\n",
      "Major already up to date: English-BSED (Year: 2024)\n",
      "Major already up to date: Fil-BSED (Year: 2023)\n",
      "Major already up to date: IE (Year: 2024)\n",
      "Major already up to date: ME (Year: 2024)\n",
      "Major already up to date: Math (Year: 2024)\n",
      "Major already up to date: Science-BSED (Year: 2024)\n",
      "Major already up to date: SocialStud-BSED (Year: 2024)\n",
      "Major update process completed.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, DateTime\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.sql import func\n",
    "import pandas as pd\n",
    "\n",
    "# Create a session\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "def get_college_data(df):\n",
    "    df = df[~df['Department'].isin(['GS', 'JHS', 'SHS', 'HAUSPELL', 'HAU', 'MA'])]\n",
    "    df = df[~df['Major'].isin(['TOTAL', 'GRAND TOTAL'])]\n",
    "    df = df.drop(df.loc[:, \"Grade_1\":\"Grade_12\"].columns, axis=1)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def clean_college_data(df):\n",
    "    # Step 1: Replace similar majors\n",
    "    for old_majors, new_major in similar_majors_dict.items():\n",
    "        df['Major'] = df['Major'].replace(old_majors, new_major[0])\n",
    "     \n",
    "    # Step 4: Drop unwanted majors\n",
    "    df = df[~df['Major'].isin(drop_majors)]\n",
    "        \n",
    "    # Step 6: Combine majors with the same name within the same semester\n",
    "    df = df.groupby(['Start_Year', 'Semester',\n",
    "                     'Major', 'Department'], as_index=False).sum([\"1st_Year\", \"2nd_Year\", \"3rd_Year\",\n",
    "                                                                  \"4th_Year\", \"5th_Year\", \"TOTAL\"])\n",
    "    return df\n",
    "\n",
    "# Load the enrollment data\n",
    "enrollment_df = pd.read_sql_table('enrollment', engine)\n",
    "\n",
    "# Apply the get_college_data function\n",
    "college_data = get_college_data(enrollment_df)\n",
    "college_data = clean_college_data(college_data)\n",
    "\n",
    "# Get unique combinations of Major and Department with the latest year\n",
    "unique_majors = college_data.groupby(['Major', 'Department'])['Start_Year'].max().reset_index()\n",
    "\n",
    "# Get the list of existing majors from the database\n",
    "existing_majors = session.query(Major.name, Major.latest_year).all()\n",
    "existing_majors_dict = {major[0]: major[1] for major in existing_majors}\n",
    "\n",
    "# Add or update each unique Major-Department combination in the majors table\n",
    "new_majors = []\n",
    "updated_majors = []\n",
    "for _, row in unique_majors.iterrows():\n",
    "    if row['Major'] not in existing_majors_dict:\n",
    "        new_majors.append(Major(name=row['Major'], department=row['Department'], latest_year=row['Start_Year']))\n",
    "        print(f\"Adding new major: {row['Major']} - {row['Department']} (Year: {row['Start_Year']})\")\n",
    "    elif row['Start_Year'] > existing_majors_dict[row['Major']]:\n",
    "        major = session.query(Major).filter_by(name=row['Major']).first()\n",
    "        major.latest_year = row['Start_Year']\n",
    "        updated_majors.append(major)\n",
    "        print(f\"Updating major: {row['Major']} - New latest year: {row['Start_Year']}\")\n",
    "    else:\n",
    "        print(f\"Major already up to date: {row['Major']} (Year: {existing_majors_dict[row['Major']]})\")\n",
    "\n",
    "# Bulk insert new majors and update existing ones\n",
    "if new_majors:\n",
    "    session.bulk_save_objects(new_majors)\n",
    "    print(f\"Added {len(new_majors)} new majors to the database.\")\n",
    "if updated_majors:\n",
    "    session.bulk_save_objects(updated_majors, update_changed_only=True)\n",
    "    print(f\"Updated {len(updated_majors)} existing majors in the database.\")\n",
    "\n",
    "# Commit the changes\n",
    "session.commit()\n",
    "\n",
    "# Close the session\n",
    "session.close()\n",
    "\n",
    "print(\"Major update process completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Get the current year\n",
    "current_year = datetime.datetime.now().year\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"data/Enrollment_Data.csv\")\n",
    "cpi_df = pd.read_csv(\"data/CPI_Education.csv\")\n",
    "inflation_df = pd.read_csv(\"data/Inflation_Rate.csv\")\n",
    "admission_df = pd.read_csv(\"data/Admission_Data.csv\")\n",
    "hfce_df = pd.read_csv(\"data/HFCE.csv\")\n",
    "\n",
    "df = clean_data(df)\n",
    "# Basic preprocessing\n",
    "df = df.drop(columns=[\"End_Year\"])\n",
    "\n",
    "# Function to add placeholder for current year\n",
    "def add_current_year_placeholder(df, year_column, value_columns):\n",
    "    max_year = df[year_column].max()\n",
    "    if max_year < current_year:\n",
    "        placeholder = df[df[year_column] == max_year].copy()\n",
    "        placeholder[year_column] = current_year\n",
    "        for col in value_columns:\n",
    "            placeholder[col] = float('nan')\n",
    "        df = pd.concat([df, placeholder], ignore_index=True)\n",
    "    return df.sort_values(by=[year_column]).reset_index(drop=True)\n",
    "\n",
    "# CPI data processing\n",
    "cpi_df_copy = cpi_df.copy()\n",
    "cpi_df_copy[\"Month\"] = cpi_df[\"Month\"].map({\n",
    "    \"Jan\": 1, \"Feb\": 2, \"Mar\": 3, \"Apr\": 4, \"May\": 5, \"Jun\": 6,\n",
    "    \"Jul\": 7, \"Aug\": 8, \"Sep\": 9, \"Oct\": 10, \"Nov\": 11, \"Dec\": 12,\n",
    "})\n",
    "cpi_df_copy = cpi_df_copy.dropna()\n",
    "cpi_df_copy[\"Year\"] = cpi_df_copy[\"Year\"].astype(int)\n",
    "cpi_df_copy = cpi_df_copy.groupby('Year')['CPI_Region3'].mean().reset_index()\n",
    "cpi_df_copy = add_current_year_placeholder(cpi_df_copy, \"Year\", [\"CPI_Region3\"])\n",
    "\n",
    "# Inflation data processing\n",
    "inflation_df = inflation_df[[\"Start_Year\", \"Inflation_Rate\"]].dropna()\n",
    "\n",
    "inflation_df = add_current_year_placeholder(inflation_df, \"Start_Year\", [\"Inflation_Rate\"])\n",
    "inflation_df = create_lag_features(inflation_df, group=None, target=\"Inflation_Rate\", lag_steps=1)\n",
    "inflation_df = inflation_df.drop(columns=[\"Inflation_Rate\"])\n",
    "\n",
    "# HFCE data processing\n",
    "hfce_df = hfce_df.groupby('Start_Year').mean().reset_index()\n",
    "hfce_df = add_current_year_placeholder(hfce_df, \"Start_Year\", [\"HFCE_Education\", \"HFCE\"])\n",
    "\n",
    "# Admission data processing\n",
    "admission_df = admission_df.drop(columns=[\"Number_of_Processed_Applicants\", \"Number_of_Enrolled_Applicants\"])\n",
    "admission_df[\"Department\"] = admission_df[\"Department\"].replace(\"CICT\", \"SOC\")\n",
    "admission_df = add_current_year_placeholder(admission_df, \"Start_Year\", [\"Number_of_Applicants\"])\n",
    "\n",
    "# Basic enrollment data processing\n",
    "df = df.groupby(['Start_Year', 'Semester', 'Major', 'Department'], as_index=False).sum([\"1st_Year\", \"2nd_Year\", \"3rd_Year\", \"4th_Year\", \"5th_Year\", \"Grade_12\", \"TOTAL\"])\n",
    "df = df[~df['Department'].isin(['GS', 'JHS', 'SHS', 'HAUSPELL', 'HAU', 'MA'])]\n",
    "df = df[~df['Major'].isin(['TOTAL', 'GRAND TOTAL'])]\n",
    "df = df.drop(df.loc[:, \"Grade_1\":\"Grade_11\"].columns, axis=1)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Apply the function to determine start month\n",
    "df['Start_Month'] = df['Semester'].apply(determine_start_month)\n",
    "\n",
    "# Convert numerical features to int\n",
    "numerical_features = df.drop(columns=[\"Major\", \"Department\"]).columns\n",
    "df[numerical_features] = df[numerical_features].astype(int)\n",
    "\n",
    "# Merging data\n",
    "df_encoded = df.copy()\n",
    "df_encoded = df_encoded.merge(cpi_df_copy, left_on=[\"Start_Year\"], right_on=[\"Year\"], how=\"left\")\n",
    "df_encoded = df_encoded.merge(inflation_df, on=[\"Start_Year\"], how=\"left\")\n",
    "df_encoded = df_encoded.merge(admission_df, on=[\"Department\", \"Start_Year\"], how=\"left\")\n",
    "df_encoded = df_encoded.merge(hfce_df, on=[\"Start_Year\"], how=\"left\")\n",
    "df_encoded = df_encoded.drop(columns=[\"Quarter\", \"Year\"])\n",
    "\n",
    "# Sort the DataFrame\n",
    "df_encoded = df_encoded.sort_values(by=['Start_Year', 'Start_Month'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Start_Year', 'Semester', 'Major', 'Department', '1st_Year', '2nd_Year',\n",
       "       '3rd_Year', '4th_Year', '5th_Year', 'Grade_12', 'TOTAL', 'Start_Month',\n",
       "       'CPI_Region3', 'Inflation_Rate_lag_1', 'Number_of_Applicants',\n",
       "       'HFCE_Education', 'HFCE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data has been saved to the 'processed_data' table in the database.\n"
     ]
    }
   ],
   "source": [
    "# Create the table and insert the data\n",
    "df_encoded.to_sql('processed_data', engine, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Processed data has been saved to the 'processed_data' table in the database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data/Enrollment_Data.csv\")\n",
    "cpi_df = pd.read_csv(\"data/CPI_Education.csv\")\n",
    "inflation_df = pd.read_csv(\"data/Inflation_Rate.csv\")\n",
    "admission_df = pd.read_csv(\"data/Admission_Data.csv\")\n",
    "hfce_df = pd.read_csv(\"data/HFCE.csv\")\n",
    "\n",
    "df = df.drop(columns=[\"End_Year\"])\n",
    "\n",
    "admission_df = admission_df.drop(columns=[\"Number_of_Processed_Applicants\", \"Number_of_Enrolled_Applicants\"])\n",
    "admission_df[\"Department\"] = admission_df[\"Department\"].replace(\"CICT\", \"SOC\")\n",
    "\n",
    "# Get Exponential Moving Average for Admission Number of Applicants\n",
    "# Filter the 2023 data\n",
    "admission_2023 = admission_df[admission_df[\"Start_Year\"] == 2023].copy()\n",
    "\n",
    "# Update the Start_Year to 2024\n",
    "admission_2023[\"Start_Year\"] = 2024\n",
    "\n",
    "# Set the Number_of_Applicants values to NaN for the 2024 data\n",
    "admission_2023[\"Number_of_Applicants\"] = float('nan')\n",
    "\n",
    "# Append the new 2024 data to the original admission_df\n",
    "admission_df = pd.concat([admission_df, admission_2023], ignore_index=True)\n",
    "\n",
    "# Ensure the DataFrame is sorted by 'Start_Year' and reset the index\n",
    "admission_df = admission_df.sort_values(by=['Start_Year']).reset_index(drop=True)\n",
    "\n",
    "admission_df = admission_df[admission_df[\"Start_Year\"] <= 2024]\n",
    "admission_df = create_lag_features(admission_df, group=[\"Department\"], target=\"Number_of_Applicants\", lag_steps=1)\n",
    "# admission_df = create_rolling_mean(admission_df, group=[\"Department\"], target=\"Number_of_Applicants\", window_size=3, min_periods=1)\n",
    "admission_df = create_rolling_std(admission_df, group=[\"Department\"], target=\"Number_of_Applicants\", window_size=3, min_periods=1, lag_steps=0)\n",
    "# admission_df = create_lag_features(admission_df, group=[\"Department\"], target=\"Number_of_Applicants\", window_size=3, min_periods=1, lag_steps=1)\n",
    "\n",
    "admission_df = admission_df.fillna(0)\n",
    "\n",
    "cpi_df_copy = cpi_df.copy()\n",
    "# cpi_df_copy = cpi_df_copy[cpi_df_copy[\"Year\"] < 2024]\n",
    "cpi_df_copy[\"Month\"] = cpi_df[\"Month\"].map({\n",
    "    \"Jan\": 1,\n",
    "    \"Feb\": 2,\n",
    "    \"Mar\": 3,\n",
    "    \"Apr\": 4,\n",
    "    \"May\": 5,\n",
    "    \"Jun\": 6,\n",
    "    \"Jul\": 7,\n",
    "    \"Aug\": 8,\n",
    "    \"Sep\": 9,\n",
    "    \"Oct\": 10,\n",
    "    \"Nov\": 11,\n",
    "    \"Dec\": 12,\n",
    "})\n",
    "cpi_df_copy = cpi_df_copy.dropna()\n",
    "cpi_df_copy[[\"Year\"]] = cpi_df_copy[[\"Year\"]].astype(int)\n",
    "cpi_df_copy = cpi_df_copy.groupby('Year')['CPI_Region3'].mean().reset_index()\n",
    "cpi_df_copy = create_rolling_std(cpi_df_copy, group=None, target=\"CPI_Region3\", window_size=6, lag_steps=0)\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Step 1: Replace similar majors\n",
    "for old_majors, new_major  in similar_majors_dict.items():\n",
    "    df['Major'] = df['Major'].replace(old_majors, new_major[0])\n",
    "\n",
    "# Step 2: Correct departments\n",
    "\n",
    "for major, department in incorrect_department_dict.items():\n",
    "    df.loc[df[\"Major\"]== major, 'Department'] = department\n",
    "    \n",
    "# Step 3: Rename departments\n",
    "df['Department'] = df['Department'].replace(department_dict)\n",
    "\n",
    "# Step 4: Drop unwanted majors\n",
    "df = df[~df['Major'].isin(drop_majors)]\n",
    "\n",
    "# # Step 5: Filter by threshold\n",
    "# major_counts = df['Major'].value_counts()\n",
    "# valid_majors = major_counts[major_counts >= threshold].index\n",
    "# df = df[finalDf['Major'].isin(valid_majors)]\n",
    "\n",
    "# Step 6: Combine majors with the same name within the same semester\n",
    "df = df.groupby(['Start_Year', 'Semester', 'Major', 'Department'], as_index=False).sum([\"1st_Year\", \"2nd_Year\", \"3rd_Year\", \"4th_Year\", \"5th_Year\", \"Grade_12\", \"TOTAL\"])\n",
    "\n",
    "\n",
    "shs_df = df[df[\"Department\"] == \"SHS\"].pivot_table(index=[\"Start_Year\", \"Semester\"], columns=\"Major\", values=\"Grade_12\").reset_index().fillna(0)\n",
    "shs_df[\"Start_Year\"] += 1\n",
    "shs_df = shs_df.rename(columns={col: f\"{col}_lag_1\" for col in shs_df.columns.drop([\"Start_Year\", \"Semester\"])})\n",
    "\n",
    "\n",
    "\n",
    "df = df[~df['Department'].isin(['GS', 'JHS', 'SHS', 'HAUSPELL', 'HAU', 'MA'])]\n",
    "df = df[~df['Major'].isin(['TOTAL', 'GRAND TOTAL'])]\n",
    "df = df.drop(df.loc[:, \"Grade_1\":\"Grade_11\"].columns, axis=1)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inflation_df = inflation_df[[\"Start_Year\", \"Inflation_Rate\"]]\n",
    "inflation_df_copy = inflation_df.dropna()\n",
    "inflation_2023 = inflation_df_copy[inflation_df_copy[\"Start_Year\"] == 2023].copy()\n",
    "\n",
    "# Update the Start_Year to 2024\n",
    "inflation_2023[\"Start_Year\"] = 2024\n",
    "\n",
    "# Set the Number_of_Applicants values to NaN for the 2024 data\n",
    "inflation_2023[\"Inflation_Rate\"] = float('nan')\n",
    "\n",
    "# Append the new 2024 data to the original admission_df\n",
    "inflation_df_copy = pd.concat([inflation_df_copy, inflation_2023], ignore_index=True)\n",
    "\n",
    "# Ensure the DataFrame is sorted by 'Start_Year' and reset the index\n",
    "inflation_df_copy = inflation_df_copy.sort_values(by=['Start_Year']).reset_index(drop=True)\n",
    "\n",
    "inflation_df_copy = inflation_df_copy[inflation_df_copy[\"Start_Year\"] <= 2024]\n",
    "\n",
    "# inflation_df_copy = create_lag_features(cpi_df_copy, group=None, target=\"Inflation_Rate\", lag_steps=1)\n",
    "# inflation_df_copy = create_rolling_mean(cpi_df_copy, group=None, target=\"Inflation_Rate\", window_size=6)\n",
    "inflation_df_copy = create_rolling_std(inflation_df_copy, group=None, target=\"Inflation_Rate\", window_size=3, lag_steps=1)\n",
    "inflation_df_copy = inflation_df_copy.drop(columns=[\"Inflation_Rate\"])\n",
    "\n",
    "\n",
    "# HFCE data processing\n",
    "hfce_df = hfce_df.groupby('Start_Year').mean().reset_index()\n",
    "hfce_df_copy = hfce_df.dropna()\n",
    "hfce_df_copy = create_lag_features(hfce_df_copy, group=None, target=\"HFCE_Education\", lag_steps=1)\n",
    "hfce_df_copy = create_rolling_std(hfce_df_copy, group=None, target=\"HFCE_Education\", window_size=6)\n",
    "hfce_df_copy = create_lag_features(hfce_df_copy, group=None, target=\"HFCE\", lag_steps=1)\n",
    "hfce_df_copy = create_rolling_std(hfce_df_copy, group=None, target=\"HFCE\", window_size=6)\n",
    "hfce_df_copy = hfce_df_copy.drop(columns=[\"HFCE_Education\", \"HFCE\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Apply the function and create Semester_Start column\n",
    "df['Start_Month'] = df['Semester'].apply(determine_start_month)\n",
    "\n",
    "# Get End Month after 18 weeks\n",
    "# df['End_Month'] = df['Start_Month'] + 4\n",
    "df = df.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# # Add 'Modified' column where 1st_Year is 0\n",
    "# df['Modified'] = df['1st_Year'].apply(lambda x: True if x == 0 else False)\n",
    "\n",
    "\n",
    "numerical_features =  df.drop(columns=[\"Major\", \"Department\"]).columns\n",
    "df[numerical_features] = df[numerical_features].astype(int)\n",
    "\n",
    "\n",
    "# Add CPI_Region3 to the dataframe\n",
    "df_encoded = df.copy()\n",
    "df_encoded[\"Start_Year\"].unique()\n",
    "\n",
    "\n",
    "# df_encoded = df.copy()\n",
    "# df_encoded.drop(drop_majors, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Merging data\n",
    "df_encoded = df_encoded.merge(cpi_df_copy, left_on=[\"Start_Year\"], right_on=[\"Year\"], how=\"left\")\n",
    "df_encoded = df_encoded.merge(inflation_df_copy, on=[\"Start_Year\"], how=\"left\")\n",
    "df_encoded = df_encoded.merge(admission_df, on=[\"Department\", \"Start_Year\"], how=\"left\")\n",
    "df_encoded = df_encoded.merge(hfce_df_copy, on=[\"Start_Year\"], how=\"left\")\n",
    "df_encoded = df_encoded.drop(columns=[\"Year\"])\n",
    "\n",
    "\n",
    "# Lagged Features\n",
    "# Calculate Exponential Moving Average (EMA)\n",
    "ema_df = pd.DataFrame()\n",
    "# ema_df['Inflation_Rate_EMA'] = df_encoded.groupby(['Start_Year', 'Major'])['Inflation_Rate'].transform(lambda x: x.ewm(span=2, adjust=False).mean())\n",
    "\n",
    "# Ensure DataFrame is sorted by 'Start_Year' and index\n",
    "df_encoded.sort_values(by=['Start_Year', 'Semester'], inplace=True)\n",
    "\n",
    "df_encoded = create_lag_features(df_encoded, lag_steps=1)\n",
    "# df_encoded = create_rolling_mean(df_encoded, lag_steps=1, window_size=3)\n",
    "df_encoded = create_rolling_std(df_encoded, lag_steps=1, window_size=3)\n",
    "\n",
    "# df_encoded = create_lag_features(df_encoded, lag_steps=2, target=\"Grade_12\")\n",
    "# df_encoded = create_rolling_std(df_encoded, lag_steps=2, window_size=3, target=\"Grade_12\")\n",
    "\n",
    "df_encoded = create_lag_features(df_encoded, lag_steps=1, target=\"TOTAL\")\n",
    "\n",
    "# df_encoded = create_rolling_std(df_encoded, window_size=3, target=\"2nd_Year\")\n",
    "# df_encoded = create_rolling_std(df_encoded, window_size=3, target=\"3rd_Year\")\n",
    "# df_encoded = create_rolling_std(df_encoded, window_size=3, target=\"4th_Year\")\n",
    "\n",
    "\n",
    "\n",
    "# df_encoded = df_encoded.drop(columns=[\"Number_of_Applicants\", \"CPI_Region3\"])\n",
    "\n",
    "\n",
    "\n",
    "# ema_df['1st_Year_EMA'] = df_encoded.groupby('Major')['1st_Year'].transform(lambda x: x.ewm(span=2, adjust=False).mean())\n",
    "# ema_df['2nd_Year_EMA'] = df_encoded.groupby('Major')['2nd_Year'].transform(lambda x: x.ewm(span=2, adjust=False).mean())\n",
    "# ema_df['3rd_Year_EMA'] = df_encoded.groupby('Major')['3rd_Year'].transform(lambda x: x.ewm(span=2, adjust=False).mean())\n",
    "# ema_df['4th_Year_EMA'] = df_encoded.groupby('Major')['4th_Year'].transform(lambda x: x.ewm(span=2, adjust=False).mean())\n",
    "\n",
    "#df_encoded = df_encoded.dropna()\n",
    "\n",
    "\n",
    "# Assume df_train is your training dataframe\n",
    "# One-hot encode departments and majors\n",
    "dept_encoder = OneHotEncoder(sparse_output=False)\n",
    "major_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "dept_encoded = dept_encoder.fit_transform(df_encoded[['Department']])\n",
    "major_encoded = major_encoder.fit_transform(df_encoded[['Major']])\n",
    "\n",
    "# Perform PCA\n",
    "dept_pca = PCA(n_components=2, random_state=24)\n",
    "major_pca = PCA(n_components=2, random_state=24)\n",
    "\n",
    "department_principalComponents = dept_pca.fit_transform(dept_encoded)\n",
    "department_principalDf = pd.DataFrame(data = department_principalComponents, columns = [f\"Department_PC{i+1}\" for i in range(2)])\n",
    "\n",
    "major_principalComponents = major_pca.fit_transform(major_encoded)\n",
    "major_principalDf = pd.DataFrame(data = major_principalComponents, columns = [f\"Major_PC{i+1}\" for i in range(2)])\n",
    "\n",
    "\n",
    "# Save the encoders and PCA objects\n",
    "joblib.dump(dept_encoder, 'data/dept_encoder.pkl')\n",
    "joblib.dump(major_encoder, 'data/major_encoder.pkl')\n",
    "joblib.dump(dept_pca, 'data/dept_pca.pkl')\n",
    "joblib.dump(major_pca, 'data/major_pca.pkl')\n",
    "\n",
    "\n",
    "# department_df = pd.get_dummies(df_encoded[[\"Department\"]])\n",
    "# major_df = pd.get_dummies(df_encoded[[\"Major\"]])\n",
    "# # df_encoded = pd.concat([df_encoded, major_df], axis=1)\n",
    "\n",
    "\n",
    "# # Dimensionality Reduction for Major\n",
    "# pca = PCA(n_components=2, random_state=24)\n",
    "# department_principalComponents = pca.fit_transform(department_df)\n",
    "# department_principalDf = pd.DataFrame(data = department_principalComponents, columns = [f\"Department_PC{i+1}\" for i in range(2)])\n",
    "\n",
    "# major_principalComponents = pca.fit_transform(major_df)\n",
    "# major_principalDf = pd.DataFrame(data = major_principalComponents, columns = [f\"Major_PC{i+1}\" for i in range(2)])\n",
    "\n",
    "\n",
    "finalDf = pd.concat([df_encoded, department_principalDf, major_principalDf], axis = 1)\n",
    "\n",
    "# finalDf = finalDf.dropna()\n",
    "\n",
    "# Drop major with less than 10 students overall\n",
    "major_counts = finalDf.groupby('Major')['1st_Year'].sum()\n",
    "valid_majors = major_counts[major_counts >= 10].index\n",
    "finalDf = finalDf[finalDf['Major'].isin(valid_majors)]\n",
    "\n",
    "finalDf = finalDf.merge(shs_df, on=[\"Start_Year\", \"Semester\"], how=\"left\")\n",
    "finalDf = finalDf.drop(columns=[\"CPI_Region3\", \"Number_of_Applicants\"])\n",
    "\n",
    "\n",
    "# Step 1: Group by year and major to get the sum of students in each major for each year\n",
    "grouped = finalDf.groupby(['Start_Year', 'Semester', 'Major'])['1st_Year_lag_1'].sum().reset_index()\n",
    "\n",
    "# Step 2: Calculate the total number of students for each year\n",
    "total_students_per_year = grouped.groupby(['Start_Year', 'Semester'])['1st_Year_lag_1'].sum().reset_index()\n",
    "total_students_per_year.rename(columns={'1st_Year_lag_1': 'Total_1st_Year_Students_lag_1'}, inplace=True)\n",
    "\n",
    "# Step 3: Merge the total students per year with the grouped data\n",
    "distribution_df = pd.merge(grouped, total_students_per_year, on=['Start_Year', 'Semester'])\n",
    "\n",
    "# Step 4: Calculate the percentage distribution of each major\n",
    "distribution_df['Percentage_Distribution_lag_1'] = (distribution_df['1st_Year_lag_1'] / distribution_df['Total_1st_Year_Students_lag_1']) * 100\n",
    "\n",
    "\n",
    "finalDf = finalDf.merge(distribution_df.drop(columns=[\"1st_Year_lag_1\"]), on=['Start_Year', 'Semester', 'Major'])\n",
    "\n",
    "\n",
    "# # Create multiple columns filled with 0s\n",
    "# for i in range(3, 5):\n",
    "#     finalDf[f\"Grade_12_lag_{i}\"] = 1\n",
    "    \n",
    "\n",
    "finalDf = finalDf.sort_values(by=['Start_Year', 'Start_Month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "747"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDf_copy = finalDf.fillna(0).drop_duplicates()\n",
    "finalDf_copy.to_sql('processed_data', engine, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAUTION\n",
    "DO NOT RUN THIS CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throw error to avoid run all to here\n",
    "raise Exception(\"Execution stopped to prevent running the irreversible code.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear majors Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sessionmaker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create a new session\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m Session \u001b[38;5;241m=\u001b[39m \u001b[43msessionmaker\u001b[49m(bind\u001b[38;5;241m=\u001b[39mengine)\n\u001b[0;32m      5\u001b[0m session \u001b[38;5;241m=\u001b[39m Session()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Delete all rows from the majors table\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sessionmaker' is not defined"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "# Create a new session\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "try:\n",
    "    # Delete all rows from the majors table\n",
    "    session.execute(text(\"DELETE FROM majors\"))\n",
    "    \n",
    "    # Commit the transaction\n",
    "    session.commit()\n",
    "    print(\"All rows have been deleted from the majors table.\")\n",
    "except Exception as e:\n",
    "    # If an error occurs, rollback the changes\n",
    "    session.rollback()\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the session\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete majors Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'majors' table has been successfully dropped.\n",
      "Table deletion process completed.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, MetaData\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# Create a MetaData instance\n",
    "metadata = MetaData()\n",
    "\n",
    "# Reflect the existing tables\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "\n",
    "try:\n",
    "    # Check if the majors table exists\n",
    "    if 'majors' in metadata.tables:\n",
    "        # Drop the majors table\n",
    "        metadata.tables['majors'].drop(engine)\n",
    "        print(\"The 'majors' table has been successfully dropped.\")\n",
    "    else:\n",
    "        print(\"The 'majors' table does not exist.\")\n",
    "    \n",
    "    # Commit the transaction\n",
    "    session.commit()\n",
    "except Exception as e:\n",
    "    # If an error occurs, rollback the changes\n",
    "    session.rollback()\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the session\n",
    "    session.close()\n",
    "\n",
    "print(\"Table deletion process completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete processed_data Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'processed_data' table does not exist.\n",
      "Table deletion process completed.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "# Create a MetaData instance\n",
    "metadata = MetaData()\n",
    "\n",
    "# Reflect the existing tables\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "# Create a session\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "try:\n",
    "    # Check if the processed_data table exists\n",
    "    if 'processed_data' in metadata.tables:\n",
    "        # Drop the processed_data table\n",
    "        metadata.tables['processed_data'].drop(engine)\n",
    "        print(\"The 'processed_data' table has been successfully dropped.\")\n",
    "    else:\n",
    "        print(\"The 'processed_data' table does not exist.\")\n",
    "    \n",
    "    # Commit the transaction\n",
    "    session.commit()\n",
    "except Exception as e:\n",
    "    # If an error occurs, rollback the changes\n",
    "    session.rollback()\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Close the session\n",
    "    session.close()\n",
    "\n",
    "print(\"Table deletion process completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enrollment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
